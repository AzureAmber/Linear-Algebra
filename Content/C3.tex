\newpage

\section[Day 3: Orthogonality]{ Orthogonality }

\subsection{ Orthogonality }

    \begin{definition}{Dot Product}{16cm}
        Let x,y $\in$ $\mathbb{R}^n$. Then the {\color{lblue} dot product}
        of x and y:

        \hspace{0.5cm}
        $x \cdot y$ = $x_1y_1 + ... + x_ny_n$ = $\sum_{i=1}^n x_iy_i$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Properties of the Dot Product}{16cm}
        Let x,y,z $\in$ $\mathbb{R}^n$ and c $\in$ $\mathbb{R}$.
    \end{wtheorem}

    \begin{enumerate}[label=(\alph*), leftmargin=2cm, itemsep=0.1cm]
        \item {\color{lgreen} Positive Definite}
        
            \hspace{0.5cm}
            $x \cdot x$ $\geq$ 0 where $x \cdot x$ = 0
            if and only if x = 0

            \begin{proof}[15cm]
                $x \cdot x$
                = $x_1x_1 + ... + x_nx_n$
                = $x_1^2 + ... + x_n^2$
                $\geq$ 0

                If 0 = $x \cdot x$ = $x_1^2 + ... + x_n^2$,
                then each $x_i$ = 0 so x = 0.

                If x = 0, then $x \cdot x$ = $x_1^2 + ... + x_n^2$
                = $0 + ... + 0$ = 0.
            \end{proof}

        \item {\color{lgreen} Symmetry}
        
            \hspace{0.5cm}
            $x \cdot y$ = $y \cdot x$

            \begin{proof}[15cm]
                $x \cdot y$
                = $x_1y_1 + ... + x_ny_n$
                = $y_1x_1 + ... + y_nx_n$
                = $y \cdot x$
            \end{proof}
        
        \item {\color{lgreen} Scalar Multiplication}

            \hspace{0.5cm}
            $(cx) \cdot y$ = $c(x \cdot y)$ = $x \cdot (cy)$

            \begin{proof}[15cm]
                $(cx) \cdot y$
                = $\sum_{i=1}^n cx_iy_i$
                = $c\sum_{i=1}^n x_iy_i$
                = $c(x \cdot y)$
                = $\sum_{i=1}^n x_i(cy_i)$
                = $x \cdot (cy)$
            \end{proof}

        \item {\color{lgreen} Distributivity}

            \hspace{0.5cm}
            $z \cdot (x+y)$
            = $(z \cdot x) + (z \cdot y)$
            \hspace{1cm}
            $(x + y) \cdot z$
            = $(x \cdot z) + (y \cdot z)$

            \begin{proof}[15cm]
                $z \cdot (x + y)$
                = $\sum_{i=1}^n z_i(x_i+y_i)$
                = $\sum_{i=1}^n z_ix_i$ + $\sum_{i=1}^n z_iy_i$
                = $(z \cdot x) + (z \cdot y)$

                $(x + y) \cdot z$
                = $\sum_{i=1}^n (x_i+y_i)z_i$
                = $\sum_{i=1}^n x_iz_i$ + $\sum_{i=1}^n y_iz_i$
                = $(x \cdot z) + (y \cdot z)$
            \end{proof}
    \end{enumerate}

    \vspace{0.5cm}



    \begin{wtheorem}{Dot Product: Length Property}{16cm}
        Let x $\in$ $\mathbb{R}^n$. Then, $x \cdot x$ = $|x|^2$.
    \end{wtheorem}

    \begin{proof}
        $x \cdot x$
        = $x_1x_1 + ... + x_nx_n$
        = $\sum_{i=1}^n x_i^2$
        = $(\sqrt{\sum_{i=1}^n x_i^2})^2$
        = $|x|^2$.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Dot Product: Cancellation Property}{16cm}
        Let x,y $\in$ $\mathbb{R}^n$.
        Then, x = y if and only if $x \cdot z$ = $y \cdot z$
        for every z $\in$ $\mathbb{R}^n$.
    \end{wtheorem}

    \begin{proof}
        Suppose x = y. Then $x_i$ = $y_i$ for i = \{1,...,n\}:

        \hspace{0.5cm}
        $x \cdot z$
        = $x_1z_1 + ... + x_nz_n$
        = $y_1z_1 + ... + y_nz_n$
        = $y \cdot z$

        \vspace{0.2cm}

        Suppose $x \cdot z$ = $y \cdot z$ for every z $\in$ $\mathbb{R}^n$.
        Then, $z \cdot (x-y)$ = 0. Let z = $x - y$. Then:

        \hspace{0.5cm}
        0 = $z \cdot (x-y)$
        = $(x-y) \cdot (x-y)$

        Thus, x - y = 0 so x = y.
    \end{proof}

    \newpage



    \begin{definition}{Transpose}{16cm}
        Let x $\in$ $\mathbb{R}^n$
        where x =
        $\begin{bmatrix}
            x_1 \\
            \vdots \\
            x_n
        \end{bmatrix}$.
        Then, the {\color{lblue} transpose} of x:

        \hspace{0.5cm}
        $x^T$ =
        $\begin{bmatrix}
            x_1 & ... & x_n
        \end{bmatrix}$

        Let A $\in$ $M_{m \times n}(\mathbb{R})$
        where A =
        $\begin{bmatrix}
            \color{red} a_{11} & \color{red} a_{12} & \hdots & \color{red} a_{1n} \\
            \color{blue} a_{21} & \color{blue} a_{22}
                & \hdots & \color{blue} a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \color{green} a_{m1} & \color{green} a_{m2}
                & \hdots & \color{green} a_{mn} 
        \end{bmatrix}$.
        Then, the transpose of A:

        \hspace{0.5cm}
        $A^T$ =
        $\begin{bmatrix}
            \color{red} a_{11} & \color{blue} a_{21}
                & \hdots & \color{green} a_{m1} \\
            \color{red} a_{12} & \color{blue} a_{22}
                & \hdots & \color{green} a_{m2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \color{red} a_{1n} & \color{blue} a_{2n} 
                & \hdots & \color{green} a_{mn} 
        \end{bmatrix}$
        $\in$ $M_{n \times m}(\mathbb{R})$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Dot Product: Transpose Property}{16cm}
        Let x,y $\in$ $\mathbb{R}^n$. Then:

        \hspace{0.5cm}
        $x \cdot y$ = $x^T y$ = $y^T x$
    \end{wtheorem}

    \begin{proof}
        $x \cdot y$
        = $x_1y_1 + ... + x_ny_n$ =
        $\begin{bmatrix}
            x_1 & ... & x_n
        \end{bmatrix}
        \begin{bmatrix}
            y_1 \\
            \vdots \\
            y_n
        \end{bmatrix}$
        = $x^T y$

        $x \cdot y$ = $y \cdot x$ = $y^T x$
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Transpose of Matrix Product}{16cm}
        Let A $\in$ $M_{m \times n}(\mathbb{R})$
        and B $\in$ $M_{t \times m}(\mathbb{R})$.
        Then, $(BA)^T$ $\in$ $M_{n \times t}(\mathbb{R})$ where:

        \hspace{0.5cm}
        $(BA)^T$ = $A^T B^T$
    \end{wtheorem}

    \begin{proof}
        $(BA)^T$ =
        $(\begin{bmatrix}
            b_{11} & b_{12} & \hdots & b_{1m} \\
            b_{21} & b_{22} & \hdots & b_{2m} \\
            \vdots & \vdots & \ddots & \vdots \\
            b_{t1} & b_{t2} & \hdots & b_{tm}
        \end{bmatrix}
        \begin{bmatrix}
            a_{11} & a_{12} & \hdots & a_{1n} \\
            a_{21} & a_{22} & \hdots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \hdots & a_{mn}
        \end{bmatrix})^T$
        
        \hspace{1.15cm}
        =
        $(\begin{bmatrix}
            \sum_{k=1}^m b_{1m}a_{m1}
                & \sum_{k=1}^m b_{1m}a_{m2}
                & \hdots
                & \sum_{k=1}^m b_{1m}a_{mn} \\
            \sum_{k=1}^m b_{2m}a_{m1}
                & \sum_{k=1}^m b_{2m}a_{m2}
                & \hdots
                & \sum_{k=1}^m b_{2m}a_{mn} \\
            \vdots & \vdots & \ddots & \vdots \\
            \sum_{k=1}^m b_{tm}a_{m1}
                & \sum_{k=1}^m b_{tm}a_{m2}
                & \hdots
                & \sum_{k=1}^m b_{tm}a_{mn} \\
        \end{bmatrix})^T$
        
        \hspace{1.15cm}
        =
        $\begin{bmatrix}
            \sum_{k=1}^m b_{1m}a_{m1}
                & \sum_{k=1}^m b_{2m}a_{m1}
                & \hdots
                & \sum_{k=1}^m b_{tm}a_{m1} \\
            \sum_{k=1}^m b_{1m}a_{m2}
                & \sum_{k=1}^m b_{2m}a_{m2}
                & \hdots
                & \sum_{k=1}^m b_{tm}a_{m2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \sum_{k=1}^m b_{1m}a_{mn}
                & \sum_{k=1}^m b_{2m}a_{mn}
                & \hdots
                & \sum_{k=1}^m b_{tm}a_{mn} \\
        \end{bmatrix}$
        $\in$ $M_{n \times t}(\mathbb{R})$ 

        \hspace{1.15cm}
        =
        $\begin{bmatrix}
            a_{11} & a_{21} & \hdots & a_{m1} \\
            a_{12} & a_{22} & \hdots & a_{m2} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{1n} & a_{2n} & \hdots & a_{mn}
        \end{bmatrix}
        \begin{bmatrix}
            b_{11} & b_{21} & \hdots & b_{t1} \\
            b_{12} & b_{22} & \hdots & b_{t2} \\
            \vdots & \vdots & \ddots & \vdots \\
            b_{1m} & b_{2m} & \hdots & b_{tm}
        \end{bmatrix}$
        = $A^T B^T$
    \end{proof}

    \newpage



    \begin{corollary}{$(Ax) \cdot y$ = $x \cdot (A^Ty)$}{16cm}
        For A $\in$ $M_{m \times n}(\mathbb{R})$ and x $\in$ $\mathbb{R}^n$,
        y $\in$ $\mathbb{R}^m$, then $A^T$ $\in$ $M_{m \times n}(\mathbb{R})$
        is unique such that:

        \hspace{0.5cm}
        $(Ax) \cdot y$
        = $x \cdot (A^Ty)$
    \end{corollary}

    \begin{proof}
        Since Ax $\in$ $\mathbb{R}^m$, then:

        \hspace{0.5cm}
        $(Ax) \cdot y$
        = $(Ax)^Ty$
        = $(x^T A^T) y$
        = $x^T (A^T y)$
        = $x \cdot (A^T y)$

        Suppose there is a B $\in$ $M_{n \times m}(\mathbb{R})$
        such that $(Ax) \cdot y$ = $x \cdot (By)$.
        
        Then for any x $\in$ $\mathbb{R}^n$,
        y $\in$ $\mathbb{R}^m$:

        \hspace{0.5cm}
        $x \cdot (By)$
        = $(Ax) \cdot y$
        = $x \cdot (A^T y)$

        By {\color{red} theorem 3.1.4},
        $B y$ = $A^T y$ so B = $A^T$.
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Orthogonality}{16cm}
        Vectors x,y $\in$ $\mathbb{R}^n$
        are {\color{lblue} orthogonal} (i.e. perpendicular)
        if $x \cdot y$ = 0
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Pythagorean Theorem in Higher Dimensions}{16cm}
        Let x,y $\in$ $\mathbb{R}^n$. Then the following are equivalent:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item $x \cdot y$ = 0
            
            \item $|x|^2 + |y|^2$ = $|x+y|^2$
            
            \item If n = 2 or 3 and x,y $\not =$ 0, then x and y
                are perpendicular
        \end{enumerate}
    \end{wtheorem}

    \begin{proof}
        Note 
        $|x+y|^2$
        = $(x+y) \cdot (x+y)$
        = $(x \cdot x) + (x \cdot y) + (y \cdot x) + (y \cdot y)$
        = $|x|^2 + |y|^2 + 2(x \cdot y)$.

        \vspace{0.2cm}

        Suppose $x \cdot y$ = 0.
        Then, $|x|^2 + |y|^2$ = $|x+y|^2$.

        \vspace{0.2cm}

        Suppose $|x|^2 + |y|^2$ = $|x+y|^2$.
        Then $2(x \cdot y)$ = 0 so $x \cdot y$ = 0.

        \vspace{0.2cm}

        Suppose $|x|^2 + |y|^2$ = $|x+y|^2$.
        By the regular definiton of the pythagorean theorem in
        $\mathbb{R}^2,\mathbb{R}^3$, x, y, and x+y form a right triangle
        so x and y form a right angle and thus, perpendicular.
        Note if x = 0, then $|x|^2 + |y|^2$ = $|x+y|^2$
        becomes $|y|^2$ = $|y|^2$ which is true, but has nothing to do with
        right triangles so exclude the case when x = 0. Similarily, exclude y = 0.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{$x \cdot y$ = $|x| |y| \cos(\theta)$}{16cm}
        Let x,y $\in$ $\mathbb{R}^n$. Then:

        \hspace{0.5cm}
        $x \cdot y$ = $|x| |y| \cos(\theta)$

        where $\theta$ $\in$ $[,\pi]$ is the angle between x and y
    \end{wtheorem}

    \begin{proof}
        Since x, y, and x-y form a triangle, by the Law of Cosine:

        \hspace{0.5cm}
        $|x-y|^2$ = $|x|^2 + |y|^2 - 2|x| \ |y| \cos(\theta)$
        
        where $\theta$ $\in$ $[0,\pi]$ is the angle between x and y. Since:

        \hspace{0.5cm}
        $|x-y|^2$
        = $(x - y) \cdot (x - y)$
        = $x \cdot x + y \cdot y - 2(x \cdot y)$
        = $|x|^2 + |y|^2 - 2(x \cdot y)$

        then $x \cdot y$ = $|x| \ |y| \cos(\theta)$.
    \end{proof}

    \newpage





\subsection{ Orthogonal Basis }

    \begin{wtheorem}{Orthogonal Projection}{16cm}
        The {\color{lblue} orthogonal projection} of x $\in$ $\mathbb{R}^n$ onto
        y $\in$ $\mathbb{R}^n$ is the component of x parallel to y:

        \hspace{0.5cm}
        $\text{proj}_yx$ = $\frac{x \cdot y}{|y|^2}y$

        Also, $\text{proj}_yx$ $\in$ span(y)
        where $x - \text{proj}_yx$ and span(y) are orthogonal.
    \end{wtheorem}

    \begin{proof}
        Since $\text{proj}_yx$ is parallel to y, let
        $\text{proj}_yx$ = cy for some c $\in$ $\mathbb{R}$.

        Let $x^{\perp}$ be the orthogonal component of x to y.
        Thus, x = $\text{proj}_yx$ + $x^{\perp}$ = cy + $x^{\perp}$.

        Since $x^{\perp}$ is orthogonal to y, then:

        \hspace{0.5cm}
        $x \cdot y$
        = $(cy + x^{\perp}) \cdot y$
        = $cy \cdot y + x^{\perp} \cdot y$
        = $c y \cdot y$
        = $c|y|^2$

        Thus, c = $\frac{x \cdot y}{|y|^2}$
        so $\text{proj}_yx$ = cy = $\frac{x \cdot y}{|y|^2}y$
        $\in$ span(y).
        Let a $\in$ $\mathbb{R}$.

        \hspace{0.5cm}
        $(x - \text{proj}_yx) \cdot ay$
        = $(x - \frac{x \cdot y}{|y|^2}y) \cdot ay$
        = $(x \cdot ay) - \frac{x \cdot y}{|y|^2}(y \cdot ay)$

        \hspace{3.6cm}
        = $a(x \cdot y) - a\frac{x \cdot y}{|y|^2}|y|^2$
        = $a(x \cdot y) - a(x \cdot y)$
        = 0

        Thus, $x - \text{proj}_yx$ and span(y) are orthogonal
        so $x - \text{proj}_yx$ and $\text{proj}_yx$ are orthogonal.
    \end{proof}

    \vspace{0.5cm}



    \begin{example}
        Let x = (-4,5,7) and y = (2,-4,1). Find the vector decomposition
        of x onto y.
    \end{example}

    \begin{tbox}
        Parallel component:
        \hspace{0.5cm}
        $\text{proj}_yx$
        = $\frac{(-4*2 + 5*-4 + 7*1)}{2^2 + (-4)^2 + 1^2}(2,-4,1)$
        = $\frac{-21}{21}(2,-4,1)$ = (-2,4,-1)

        Orthogonal component:
        \hspace{0.5cm}
        x - $\text{proj}_yx$
        = (-4,5,7) - (-2,4,-1)
        = (-2,1,8)
    \end{tbox}

    \vspace{0.5cm}



    \begin{wtheorem}{Cauchy-Schwarz Inequality}{16cm}
        For x,y $\in$ $\mathbb{R}^n$,
        $|x \cdot y|$ $\leq$ $|x| |y|$
    \end{wtheorem}

    \begin{proof}
        Let x = proj$_yx$ + $x^{\perp}$ = cy + $x^{\perp}$
        where $x^{\perp}$ is the orthogonal component of x to y and
        proj$_yx$ = cy is the parallel component of x to y.
        By {\color{red} theorem 3.2.1}, then c = $\frac{x \cdot y}{|y|^2}$.

        \hspace{0.5cm}
        $|x|^2$
        = $|cy+x^{\perp}|^2$
        = $|cy|^2 + |x^{\perp}|^2$
        = $(\frac{x \cdot y}{|y|^2})^2 |y|^2 + |x^{\perp}|^2$

        \hspace{0.5cm}
        $|x|^2 |y|^2$
        = $|y|^2(\frac{x \cdot y}{|y|^2})^2 |y|^2 + |y|^2 |x^{\perp}|^2$
        = $(x \cdot y)^2 + |y|^2 |x^{\perp}|^2$

        Since $|y|^2 |x^{\perp}|^2$ $\geq$ 0, then
        $(x \cdot y)^2$ $\leq$ $|x|^2 |y|^2$
        so $|x \cdot y|$ $\leq$ $|x| |y|$.
    \end{proof}

    \vspace{0.5cm}



    \begin{corollary}{Triangle Inequality}{16cm}
        For x,y $\in$ $\mathbb{R}^n$,
        $||x+y||$ $\leq$ $||x|| + ||y||$
    \end{corollary}

    \begin{proof}
        $|x+y|^2$
        = $(x+y) \cdot (x+y)$
        = $x \cdot x + y \cdot y + 2(x \cdot y)$
        = $|x|^2 + |y|^2 +2(x \cdot y)$

        \hspace{1.35cm}
        $\leq$ $|x|^2 + |y|^2 +2|x \cdot y|$
        $\leq$ $|x|^2 + |y|^2 +2|x| \ |y|$
        = $(|x| + |y|)^2$
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Orthonormal Vectors}{16cm}
        Vectors $v_1,...,v_k$ $\in$ $\mathbb{R}^n$
        is {\color{lblue} orthogonal} if:
        
        \hspace{0.5cm}
        $v_i \cdot v_j$ = 0 for i $\not =$ j

        Then, $v_1,...,v_k$ $\in$ $\mathbb{R}^n$
        is {\color{lblue} orthonormal} if:
        
        \hspace{0.5cm}
        $v_i \cdot v_j$ = 0 for i $\not =$ j
        \hspace{1cm}
        $|v_i|^2$ = $v_i \cdot v_i$ = 1
        \hspace{0.5cm}
        $\Leftrightarrow$
        \hspace{0.5cm}
        $|v_i|$ = 1
        for i $\in$ \{1,...,k\}
    \end{definition}

    \newpage



    \begin{wtheorem}{Orthogonal Sets are Linearly independent}{16cm}
        Let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ be orthogonal.
        Then, $v_1,...,v_k$ is linearly independent.
    \end{wtheorem}

    \begin{proof}
        Let $c_1,...,c_k$ $\in$ $\mathbb{R}$ such that
        0 = $c_1v_1 + ... + c_kv_k$.
        Since $v_i \cdot v_j$ = 0 for i $\not =$ j, then:

        \hspace{0.5cm}
        0 = $v_i \cdot 0$
        = $v_i \cdot (c_1v_1 + ... + c_kv_k)$
        = $c_i(v_i \cdot v_i)$
        = $c_i|v_i|^2$

        Since $|v_i|$ $>$ 0, then $c_i$ = 0.
        Since every $c_i$ = 0, then $v_1,...,v_k$ is linearly independent.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Orthogonal Basis}{16cm}
        For V $\subset$ $\mathbb{R}^n$, let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$
        be an orthogonal basis for V. Then, for each x $\in$ V:

        \hspace{0.5cm}
        x = $\text{proj}_{v_1}x + ... + \text{proj}_{v_k}x$
        = $(\frac{x \cdot v_1}{|v_1|^2})v_1
            + ... + (\frac{x \cdot v_k}{|v_k|^2})v_k$

        Then if $v_1,...,v_k$ is an orthonormal basis, then:

        \hspace{0.5cm}
        x = $(x \cdot v_1)v_1 + ... + (x \cdot v_k)v_k$
    \end{wtheorem}

    \begin{proof}
        Since $v_1,...,v_k$ is a basis for V, then for any x $\in$ V,
        there are $c_1,...,c_k$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        x = $c_1v_1 + ... + c_kv_k$

        Since $v_1,...,v_k$ is orthogonal, then
        $v_i \cdot v_j$ = 0 for i $\not =$ j. Thus:

        \hspace{0.5cm}
        $v_i \cdot x$ = $v_i \cdot (c_1v_1 + ... + c_kv_k)$
        = $c_i(v_i \cdot v_i)$
        = $c_i|v_i|^2$
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        $c_i$ = $\frac{x \cdot v_i}{|v_i|^2}$

        Since each $c_i$ = $\frac{x \cdot v_i}{|v_i|^2}$, then:
        
        \hspace{0.5cm}
        x = $c_1v_1 + ... + c_kv_k$
        = $(\frac{x \cdot v_1}{|v_1|^2})v_1
            + ... + (\frac{x \cdot v_k}{|v_k|^2})v_k$
        = $\text{proj}_{v_1}x + ... + \text{proj}_{v_k}x$

        Additionally, if $v_1,...,v_k$ is orthonormal,
        then each $|v_i|$ = 1. Thus,
        x = $(x \cdot v_1)v_1 + ... + (x \cdot v_k)v_k$.
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Orthogonal Complement}{16cm}
        For V $\subset$ $\mathbb{R}^n$, the {\color{lblue} orthogonal complement}
        of V:

        \hspace{0.5cm}
        $V^{\perp}$
        = \{ w $\in$ $\mathbb{R}^n$ : w $\cdot$ v = 0 for v $\in$ V \}
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Orthogonal Projection onto V $\subset$ $\mathbb{R}^n$}{16cm}
        Let V $\subset$ $\mathbb{R}^n$. Then for any x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        x = $x^{||} + x^{\perp}$

        where $x^{||}$ = $\text{proj}_Vx$,
        the {\color{lblue} orthogonal projection} of x onto V,
        is the component of x in V
        and $x^{\perp}$ is the component of x orthogonal to V
        are both unique
    \end{wtheorem}

    \begin{proof}
        Since $x^{||}$ is in V and $x^{\perp}$ is orthogonal to V,
        then $x^{||} \cdot x^{\perp}$ = 0.

        Let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ be an orthogonal basis for V.
        Then by {\color{red} theorem 3.2.6}, since $x^{||}$ $\in$ V, then:

        \hspace{0.5cm}
        $x^{||}$ = $\text{proj}_{v_1}x^{||} + ... + \text{proj}_{v_k}x^{||}$

        Suppose x = $x^{||}_1 + x^{\perp}_1$ = $x^{||}_2 + x^{\perp}_2$
        for $x^{||}_1,x^{||}_2,x^{\perp}_1,x^{\perp}_2$ $\in$ $\mathbb{R}^n$.
        Then:

        \hspace{0.5cm}
        $(x^{||}_1 - x^{||}_2) + (x^{\perp}_1 - x^{\perp}_2)$
        = $x^{||}_1 + x^{\perp}_1 - x^{||}_2 - x^{\perp}_2$
        = $x-x$ = 0

        Since $x^{||}_1,x^{||}_2$ $\in$ V
        and $x^{\perp}_1,x^{\perp}_2$ $\in$ $V^{\perp}$, then:

        \hspace{0.5cm}
        0 = $0 \cdot (x^{||}_1 - x^{||}_2)$
        = $[(x^{||}_1 - x^{||}_2) + (x^{\perp}_1 - x^{\perp}_2)]
            \cdot (x^{||}_1 - x^{||}_2)$
        = $(x^{||}_1 - x^{||}_2) \cdot (x^{||}_1 - x^{||}_2)$
        = $|x^{||}_1 - x^{||}_2|^2$

        \hspace{0.5cm}
        $|x^{||}_1 - x^{||}_2|$ = 0
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        $x^{||}_1$ = $x^{||}_2$

        \vspace{0.1cm}

        \hspace{0.5cm}
        0 = $0 \cdot (x^{\perp}_1 - x^{\perp}_2)$
        = $[(x^{||}_1 - x^{||}_2) + (x^{\perp}_1 - x^{\perp}_2)]
            \cdot (x^{\perp}_1 - x^{\perp}_2)$
        = $(x^{\perp}_1 - x^{\perp}_2) \cdot (x^{\perp}_1 - x^{\perp}_2)$
        = $|x^{\perp}_1 - x^{\perp}_2|^2$

        \hspace{0.5cm}
        $|x^{\perp}_1 - x^{\perp}_2|$ = 0
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        $x^{\perp}_1$ = $x^{\perp}_2$

        Thus, $x^{||},x^{\perp}$ are unique.
    \end{proof}

    \newpage



    \begin{wtheorem}{Gram-Schmidt Process: Creating an Orthogonal Basis}{16cm}
        For V $\subset$ $\mathbb{R}^n$, let $u_1,...,u_k$ $\in$ $\mathbb{R}^n$
        be a basis for V. Then let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        $\begin{matrix*}[l]
            v_1 = u_1 \\
            v_2 = u_2 - \text{proj}_{v_1}(u_2) \\
            v_3 = u_3 - \text{proj}_{v_1}(u_3) - \text{proj}_{v_2}(u_3) \\
            \vdots \\
            v_k = u_k - \text{proj}_{v_1}(u_k) - ... - \text{proj}_{v_{k-1}}(u_k)
        \end{matrix*}$

        Then, $v_1,...,v_k$ is an orthogonal basis for V.

        Also, $\frac{v_1}{|v_1|},...,\frac{v_k}{|v_k|}$ is an orthonormal
        basis for V.
    \end{wtheorem}

    \begin{proof}
        $v_1 \cdot v_2$
        = $u_1 \cdot (u_2 - \text{proj}_{v_1}(u_2))$
        = $u_1 \cdot (u_2 - \frac{u_2 \cdot u_1}{|u_1|^2}u_1)$
        = $u_1 \cdot u_2 - u_1 \cdot u_2$
        = 0

        Suppose for m $\leq$ k, then $v_1,...,v_{m-1}$ are orthogonal.

        Since $v_m$ =
        $u_m - \text{proj}_{v_1}(u_m) - ... - \text{proj}_{v_{m-1}}(u_m)$, where
        by {\color{red} theorem 3.2.1}, each
        $\text{proj}_{v_i}(u_m)$ = $c_iv_i$
        where $c_i$ = $\frac{u_m \cdot v_i}{|v_i|^2}$
        for i = \{1,...,m-1\}

        \hspace{0.5cm}
        $v_m$ = $u_m - c_1v_1 - ... - c_{m-1}v_{m-1}$

        \hspace{0.5cm}
        $v_i \cdot v_m$
        = $v_i \cdot (u_m - c_1v_1 - ... - c_{m-1}v_{m-1})$
        = $v_i \cdot u_m - c_i(v_i \cdot v_i)$
        = $v_i \cdot u_m - u_m \cdot v_i$
        = 0

        Thus, $v_m$ is orthgonal to any $v_i$ for i = \{1,...,m-1\}
        so $v_1,...,v_m$ is orthogonal.

        Thus, by proof by induction, $v_1,...,v_k$ is orthogonal.
        
        Similarily, $v_1$ = $u_1$ $\in$ V.
        Suppose for m $\leq$ k, then $v_1,...,v_{m-1}$ $\in$ V.

        Then, $v_m$ = $u_m - c_1v_1 - ... - c_{m-1}v_{m-1}$ $\in$ V.
        Thus, by proof by induction, $v_1,...,v_k$ $\in$ V.

        By {\color{red} theorem 3.2.5}, $v_1,...,v_k$
        are linearly independent. Since $u_1,...,u_k$ is a basis for V,
        then by {\color{orange} corollary 2.3.3}, dim(V) = k.
        Then, by {\color{red} theorem 2.3.5}, $v_1,...,v_k$
        span V and thus, form a basis for V.
        Thus, $v_1,...,v_k$ is an orthogonal basis for V.

        Since each $|\frac{v_i}{|v_i|}|$ = $\frac{1}{|v_i|}|v_i|$ = 1,
        then $\frac{v_1}{|v_1|},...,\frac{v_k}{|v_k|}$ is an orthonormal
        basis for V.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Orthogonal Projection Matrix Transformation}{16cm}
        For V $\in$ $\mathbb{R}^n$, let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$
        be an orthonormal basis for V. Then for any x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        $\text{proj}_Vx$ = $AA^T$x

        where A =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}$
        $\in$ $M_{n \times k}(\mathbb{R})$
    \end{wtheorem}

    \begin{proof}
        By {\color{red} theorem 2.3.6}:

        \hspace{0.5cm}
        $\text{proj}_Vx$
        = $(x \cdot v_1)v_1 + ... + (x \cdot v_k)v_k$
        
        \hspace{1.9cm}
        =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}
        \begin{bmatrix}
            x \cdot v_1 \\
            \vdots \\
            x \cdot v_k
        \end{bmatrix}$ =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}
        \begin{bmatrix}
            v_1^Tx \\
            \vdots \\
            v_k^Tx
        \end{bmatrix}$
        
        \hspace{1.9cm}
        =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}
        \begin{bmatrix}
            v_1^T \\
            \vdots \\
            v_k^T
        \end{bmatrix}x$ =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}
        \begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}^Tx$
    \end{proof}

    \newpage



\subsection{ Orthogonal Transformations }

    \begin{definition}{Orthogonal Transformation}{16cm}
        Linear Transformation T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        is {\color{lblue} orthogonal} if for any x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        $|T(x)|$ = $|x|$
    \end{definition}

    \vspace{0.5cm}

    

    \begin{wtheorem}{Orthgonal Transformation Equivalences}{16cm}
        Let linear transformation T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be T(x) = Ax for A $\in$ $M_{n \times n}(\mathbb{R})$.

        Then the following are equivalent:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item For any x $\in$ $\mathbb{R}^n$, then
                $|T(x)|$ = $|x|$

            \item For any x,y $\in$ $\mathbb{R}^n$, then
                $T(x) \cdot T(y)$ = $x \cdot y$

            \item $T(e_1),...,T(e_n)$ is an orthonormal basis for $\mathbb{R}^n$
            
            \item $A^TA$ = $I_{n \times n}$
            
            \item A is invertible where $A^{-1}$ = $A^T$
        \end{enumerate}
    \end{wtheorem}

    \begin{proof}
        Suppose for any x $\in$ $\mathbb{R}$, then $|T(x)|$ = $|x|$.

        Thus, for x,y $\in$ $\mathbb{R}^n$,
        then $|T(x) + T(y)|$ = $|T(x+y)|$ = $|x+y|$. Thus:

        \hspace{0.5cm}
        $|T(x) + T(y)|^2$
        = $|x+y|^2$
        = $(x+y) \cdot (x+y)$
        = $|x|^2 + |y|^2 + 2(x \cdot y)$

        \hspace{0.5cm}
        $|T(x) + T(y)|^2$
        = $(T(x) + T(y)) \cdot (T(x) + T(y))$
        
        \hspace{3.2cm}
        = $|T(x)|^2 + |T(y)|^2 + 2(T(x) \cdot T(y))$
        = $|x|^2 + |y|^2 + 2(T(x) \cdot T(y))$

        Thus, $T(x) \cdot T(y)$ = $x \cdot y$.

        \vspace{0.3cm}

        Suppose for any x,y $\in$ $\mathbb{R}^n$, then
        $T(x) \cdot T(y)$ = $x \cdot y$.

        \hspace{0.5cm}
        $T(e_i) \cdot T(e_j)$
        = $e_i \cdot e_j$ =
        $\begin{cases}
            0 & i \not = j \\
            1 & i = j
        \end{cases}$

        Thus, $T(e_1),...,T(e_n)$ are orthogonal.
        By {\color{red} theorem 3.2.5}, $T(e_1),...,T(e_n)$
        are linearly independent. Since dim($\mathbb{R}^n$) = n,
        then by {\color{red} theorem 2.3.5}, $T(e_1),...,T(e_n)$ is a basis.

        \vspace{0.3cm}

        Suppose $T(e_1),...,T(e_n)$ is an orthonormal basis for $\mathbb{R}^n$.

        Since A =
        $\begin{bmatrix}
            T(e_1) & ... & T(e_n)
        \end{bmatrix}$ and
        $T(e_i) \cdot T(e_j)$ =
        $\begin{cases}
            0 & i \not = j \\
            1 & i = j
        \end{cases}$, then:

        \hspace{0.5cm}
        $A^TA$ =
        $\begin{bmatrix}
            T(e_1)^T \\
            \vdots \\
            T(e_n)^T 
        \end{bmatrix}
        \begin{bmatrix}
            T(e_1) & ... & T(e_n)
        \end{bmatrix}$
        
        \hspace{1.5cm}
        =
        $\begin{bmatrix}
            T(e_1)^T T(e_1) & T(e_1)^T T(e_2) & \hdots & T(e_1)^T T(e_n) \\
            T(e_2)^T T(e_1) & T(e_2)^T T(e_2) & \hdots & T(e_2)^T T(e_n) \\
            \vdots & \vdots & \ddots & \vdots \\
            T(e_n)^T T(e_1) & T(e_n)^T T(e_2) & \hdots & T(e_n)^T T(e_n) \\
        \end{bmatrix}$ =
        $\begin{bmatrix}
            1 & 0 & \hdots & 0 \\
            0 & 1 & \hdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \hdots & 1 \\
        \end{bmatrix}$

        \vspace{0.3cm}

        Suppose $A^TA$ = $I_{n \times n}$.

        Then by {\color{red} theorem 1.5.11}, then A,$A^T$ are invertible
        where $A^{-1}$ = $A^T$.

        \vspace{0.5cm}

        Suppose A is invertible where $A^{-1}$ = $A^T$.
        
        Then for any x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        $|T(x)|^2$
        = $T(x) \cdot T(x)$
        = $Ax \cdot Ax$
        = $(Ax)^T(Ax)$
        = $x^TA^TAx$

        \hspace{1.9cm}
        = $x^TA^{-1}Ax$
        = $x^Tx$
        = $x \cdot x$
        = $|x|^2$

        Thus, $|T(x)|$ = $|x|$.
    \end{proof}

    \newpage



    \begin{corollary}{Inverse and Transpose of an Orthogonal matrix
    is Orthogonal}{16cm}
        Let A $\in$ $M_{n \times n}(\mathbb{R})$ be orthogonal.
        Then, $A^{-1},A^T$ are orthogonal.
    \end{corollary}

    \begin{proof}
        By {\color{red} theorem 3.3.2}, A is invertible so $A^{-1}$ is invertible.
        Since $A^{-1}$ is invertible, then $|A^{-1}(x)|$ = $|x|$
        so $A^{-1}$ is orthogonal.
        Since $A^{-1}$ = $A^T$, then $A^T$ is orthogonal.
    \end{proof}

    \vspace{0.5cm}



    \begin{corollary}{Products of Orthogonal matrices are Orthogonal}{16cm}
        Let A,B $\in$ $M_{n \times n}(\mathbb{R})$ be orthogonal.
        Then, AB is orthogonal.
    \end{corollary}

    \begin{proof}
        By {\color{red} theorem 3.3.2}, A,B are invertible.
        Then by {\color{red} theorem 1.5.13}, AB is invertible
        so AB is orthgonal.
    \end{proof}

    \vspace{0.5cm}

























