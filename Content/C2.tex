\newpage

\section[Day 2: Vector Space]{ Vector Space }

\subsection{ Span \& Independence }

    \begin{definition}{Vector Space}{16cm}
        V is a {\color{lblue} vector space} over $\mathbb{K}$ if for any
        u,v,w $\in$ V and a,b $\in$ $\mathbb{K}$:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item {\color{lgreen} Commutativity}
            
                \hspace{0.5cm}
                u + v = v + u
    
            \item {\color{lgreen} Addititive Associativity}
            
                \hspace{0.5cm}
                (u + v) + w = u + (v + w)
    
            \item {\color{lgreen} Additive Identity}
            
                There exists a unique $0_v$ $\in$ V
                such that for all v $\in$ V:
    
                \hspace{0.5cm}
                $0_v$ + v = v

            \item {\color{lgreen} Additive Inverse}
            
                For any v, there exists a unique -v $\in$ V such that:
    
                \hspace{0.5cm}
                v + (-v) = $0_v$
    
            \item {\color{lgreen} Distributivity}
            
                \hspace{0.5cm}
                a(u+v) = au + av
                \hspace{1cm}
                (a+b)u = au + bu
    
            \item {\color{lgreen} Multiplicative Associativity}
            
                \hspace{0.5cm}
                $a(bu)$ = $(ab)u$
    
            \item {\color{lgreen} Multiplicative Identity}
            
                \hspace{0.5cm}
                1u = u
        \end{enumerate}
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{0v = $0_v$, $a0_v$ = $0_v$, (-1)v = -v}{16cm}
        Let V be a vector space where v $\in$ V. Then:

        \hspace{0.5cm}
        0v = $0_v$
        \hspace{1cm}
        $a0_v$ = $0_v$
        \hspace{1cm}
        (-1)v = -v
    \end{wtheorem}

    \begin{proof}
        Since 0v = (0+0)v = 0v + 0v, then:
        
        \hspace{0.5cm}
        $0_v$ = 0v + (-0v) = 0v + 0v + (-0v) = 0v + $0_v$ = 0v
        
        Since a$0_v$
        = a($0_v$+$0_v$)
        = a$0_v$ + a$0_v$, then:

        \hspace{0.5cm}
        $0_v$ = a$0_v$ + (-a$0_v$)
        = a$0_v$ + a$0_v$ + (-a$0_v$)
        = a$0_v$ + $0_v$
        = a$0_v$

        Since $0_v$ = 0v
        = (-1+1)v
        = (-1)v + 1v
        = (-1)v + v, then:
        
        \hspace{0.5cm}
        -v
        = $0_v$ + (-v)
        = (-1)v + v + (-v)
        = (-1)v + $0_v$
        = (-1)v
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Linear Combination, Span, and Independence}{16cm}
        x $\in$ $\mathbb{R}^n$ is a {\color{lblue} linear combination}
        of $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ if there are
        $c_1,...,c_k$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        x = $c_1v_1 + ... + c_kv_k$

        The {\color{lblue} span} of $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ is the
        set of all linear combinations of $v_1,...,v_k$.

        Also, $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ are
        {\color{lblue} linearly independent}
        if none of the $v_i$ are linear combinations of the other $v_i$'s.
        Else, $v_1,...,v_k$ are {\color{lblue} linearly dependent}.
    \end{definition}

    \newpage



    \begin{wtheorem}{Remove Linearly dependent vectors to get
    Linear independence}{16cm}
        Let u $\in$ $\mathbb{R}^n$ be a linear combination of
        $v_1,...,v_k$ $\in$ $\mathbb{R}^n$. Then:

        \hspace{0.5cm}
        span($v_1,...,v_k$) = span($v_1,...,v_k$,u)

        Thus, by removing vectors that are linear
        combinations (i.e. linearly dependent vectors),
        then the resulting set of vectors will be linearly independent and
        the span is unaffected.
    \end{wtheorem}

    \begin{proof}
        Since u is a linear combination of $v_1,...,v_k$, then there are
        $c_1,...,c_k$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        u = $c_1v_1 + ... + c_kv_k$

        Let $u_1$ be a linear combination of $v_1,...,v_k$,u.
        Then there are $a_1,...,a_k,a$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        $u_1$ = $a_1v_1 + ... + a_kv_k + au$
        = $a_1v_1 + ... + a_kv_k + a(c_1v_1 + ... + c_kv_k)$
        = $(a_1+ac_1)v_1 + ... + (a_k+ac_k)v_k$

        Thus, $u_1$ $\in$ span($v_1,...,v_k$)
        so span($v_1,...,v_k$,u) $\subset$ span($v_1,...,v_k$).

        Let $u_2$ be a linear combination of $v_1,...,v_k$.
        Then there are $b_1,...,b_k,$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        $u_2$ = $b_1v_1 + ... + b_kv_k$
        = $[(b_1-c_1)v_1 + ... + (b_k-c_k)v_k] + [c_1v_1 + ... + c_kv_k]$

        \hspace{1.05cm}
        = $(b_1-c_1)v_1 + ... + (b_k-c_k)v_k + u$

        Thus, $u_2$ $\in$ span($v_1,...,v_k$,u)
        so span($v_1,...,v_k$) $\subset$ span($v_1,...,v_k$,u).
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Condition for Linear independence}{16cm}
        $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ are linearly independent
        if and only if the only solution $(c_1,...,c_k)$:

        \hspace{0.5cm}
        $c_1v_1 + ... + c_kv_k$ = 0

        is $c_1$ = ... = $c_k$ = 0.

        Note regardless of $v_1,...,v_k$, any $c_1v_1 + ... + c_kv_k$ = 0
        holds true when $(c_1,...,c_k)$ = 0 $\in$ $\mathbb{R}^k$.
        
        $(c_1,...,c_k)$ = 0 is called the {\color{lblue} trivial solution}.
        Any $(c_1,...,c_k)$ $\not =$ 0 is a {\color{lblue} nontrivial solution}.
    \end{wtheorem}

    \begin{proof}
        Suppose  $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ are linearly independent.

        Then for any $v_i$, there are no $c_1,...,c_{i-1},c_{i+1},...,c_k$
        $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        $v_i$ = $c_1v_1 + ... + c_{i-1}v_{i-1} + c_{i+1}v_{i+1} + ... + c_kv_k$

        Thus, there are no $(c_1,...,c_{i-1},c_i=-1,c_{i+1},...,c_k)$
        such that:

        \hspace{0.5cm}
        0 = $c_1v_1 + ... + c_{i-1}v_{i-1} + c_iv_i + c_{i+1}v_{i+1} + ... + c_kv_k$

        The statement holds true if the equation was multiplied by any
        non-zero number. Thus, any $(c_1,...,c_k)$ where at least one
        $c_i$ is not 0 is not a solution.
        Since $(c_1,...,c_k)$ = 0 is a solution for $c_1v_1 + ... + c_kv_k$ = 0,
        then for linearly independent $v_1,...,v_k$, then $(c_1,...,c_k)$ = 0.

        \vspace{0.2cm}

        Suppose the solution, $(c_1,...,c_k)$, to
        $c_1v_1 + ... + c_kv_k$ = 0 is only $(c_1,...,c_k)$ = 0.

        Suppose there is a linearly dependent vector, $v_i$.
        Then there are $a_1,...,a_{i-1},a_{i+1},...,a_k$ where:

        \hspace{0.5cm}
        $v_i$ = $a_1v_1 + ... + a_{i-1}v_{i-1} + a_{i+1}v_{i+1} + ... + a_kv_k$

        \hspace{0.5cm}
        0 = $a_1v_1 + ... + a_{i-1}v_{i-1} + -v_i + a_{i+1}v_{i+1} + ... + a_kv_k$

        Thus, $(a_1,...,a_{i-1},-1,a_{i+1},...,a_k)$ is
        a solution to $c_1v_1 + ... + c_kv_k$ = 0
        contradicting that $(c_1,...,c_k)$ = 0.
        Thus, there are no linearly dependent vectors.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Extending the Span of Linearly independent vectors}{16cm}
        Let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ be linearly independent.
        If v $\in$ $\mathbb{R}^n$ is not in the span($v_1,...,v_k$), then
        $v_1,...,v_k,v$ are linearly independent.
    \end{wtheorem}

    \begin{proof}
        Let $c_1,...,c_k,c$ $\in$ $\mathbb{R}$ be such that
        $c_1v_1 + ... + c_kv_k + cv$ = 0.
        Suppose c $\not =$ 0. Then:

        \hspace{0.5cm}
        $c_1v_1 + ... + c_kv_k$ = -cv
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        v = $\frac{-c_1}{c}v_1 + ... + \frac{-c_k}{c}v_k$

        Then, v is a linear combination of $v_1,...,v_k$
        and thus, v is in the span($v_1,...,v_k$)
        which is a contradiction. Thus, c = 0. Then:

        \hspace{0.5cm}
        0 = $c_1v_1 + ... + c_kv_k + cv$
        = $c_1v_1 + ... + c_kv_k$

        Since $v_1,...,v_k$ are linearly independent, then
        each $c_1$ = ... = $c_k$ = 0. Thus, $(c_1,...,c_k,c)$ = 0
        so $v_1,...,v_k,v$ are linearly independent.
    \end{proof}

    \newpage





\subsection{ Subspaces: Image \& Kernel }

    \begin{definition}{Subspaces}{16cm}
        V $\subset$ $\mathbb{R}^n$ is a {\color{lblue} subspace}
        of $\mathbb{R}^n$ if:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item {\color{lgreen} Zero Vector Existence}

                \hspace{0.5cm}
                0 $\in$ V

            \item {\color{lgreen} Closed under Addition}:
                If $v_1,v_2$ $\in$ V, then:

                \hspace{0.5cm}
                $v_1+v_2$ $\in$ V

            \item {\color{lgreen} Closed under Scalar Multiplication}:
                If v $\in$ V and c $\in$ $\mathbb{R}$, then:

                \hspace{0.5cm}
                cv $\in$ V
        \end{enumerate}
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Union of Subspaces's condition for Subspace}{16cm}
        Let U,V $\subset$ $\mathbb{R}^n$ be subspaces.
        Then, U $\cup$ V is a subspace if and only if
        U $\subset$ V or V $\subset$ U.
    \end{wtheorem}

    \begin{proof}
        Suppose U $\cup$ V is a subspace.
        Suppose U $\not \subset$ V and V $\not \subset$ U.
        Then there is a u $\in$ U where u $\not \in$ V
        and a v $\in$ V where v $\not \in$ U.
        Thus, u,v $\in$ U $\cup$ V, but u+v $\not \in$ U since
        v $\not \in$ U and u+v $\not \in$ V since u $\not \in$ V.
        Thus, u+v $\not \in$ U $\cup$ V contradicing V is a subspace.
        Thus, U $\subset$ V or V $\subset$ U.

        \vspace{0.2cm}

        If U $\subset$ V, then U $\cup$ V = V is a subspace.
        If V $\subset$ U, then U $\cup$ V = U is a subspace. 
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Intersection of Subspaces is a Subspace}{16cm}
        Let U,V $\subset$ $\mathbb{R}^n$ be subspaces.
        Then, U $\cap$ V is a subspace.
    \end{wtheorem}

    \begin{proof}
        Let x,y $\in$ U $\cap$ V and a,b $\in$ $\mathbb{R}$.
        Then x,y $\in$ U,V.
        Since U and V are subspaces, then ax+by $\in$ U,V.
        Thus, ax+by $\in$ U $\cap$ V so U $\cap$ V is a subspace.
    \end{proof}

    \vspace{0.5cm}

    \begin{definition}{Image and Kernel}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$
        be the linear transformation T(x) = Ax.

        The {\color{lblue} image} of T is the set of all Ax $\in$ $\mathbb{R}^m$
        where x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        im(T) = im(A) = \{ A(x) $|$ x $\in$ $\mathbb{R}^n$ \}

        The {\color{lblue} kernel} of T is the set of all x $\in$ $\mathbb{R}^n$
        such that Ax = 0:

        \hspace{0.5cm}
        ker(T) = ker(A) = \{ x $|$ Ax = 0 \}
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Im(A) is a Subspace that spans the columns of A}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$
        be the linear transformation T(x) = Ax.
        
        Then, im(A) is a subspace and im(A) = span(A).
    \end{wtheorem}

    \begin{proof}
        Since Ax = $A_1x_1 + ... + A_nx_n$
        for x = $(x_1,...,x_n)$ $\in$ $\mathbb{R}^n$
        where $A_1,...,A_n$ are the columns of A:

        \hspace{0.5cm}
        im(A) = \{ Ax $|$ x $\in$ $\mathbb{R}^n$ \}
        = \{ $x_1A_1 + ... + x_nA_n$ $|$ $x_1,...,x_n$ $\in$ $\mathbb{R}$ \}
        = span($A_1,...,A_n$)

        Since A0 = 0, then 0 $\in$ im(A).
        Let u,v $\in$ im(A) and a,b $\in$ $\mathbb{R}$.

        Then there are $a_1,...,a_n$ $\in$ $\mathbb{R}$ and
        $b_1,...,b_n$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        u = $a_1A_1 + ... + a_nA_n$
        \hspace{1cm}
        v = $b_1A_1 + ... + b_nA_n$

        Thus, au+bv = $(aa_1+bb_1)A_1 + ... + (aa_n+bb_n)A_n$
        $\in$ span($A_1,...,A_n$) = im(A).
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{ker(A) is a Subspace}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$
        be the linear transformation T(x) = Ax.
        Then, ker(A) is a subspace.
    \end{wtheorem}

    \begin{proof}
        Since A0 = 0, then 0 $\in$ ker(A).
        Let $x_1,x_2$ $\in$ ker(A) and a,b $\in$ $\mathbb{R}$
        so A$x_1$ = A$x_2$ = 0.
        
        Then, A($ax_1 + bx_2$)
        = aA$(x_1)$ + bA$(x_2)$
        = a0 + b0 = 0
        so $ax_1+bx_2$ $\in$ ker(A).
    \end{proof}

    \newpage



    \begin{wtheorem}{Relationship between the Kernel and Linear independence}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$
        be the linear transformation T(x) = Ax where A =
        $\begin{bmatrix}
            A_1 & ... & A_m
        \end{bmatrix}$.

        Then, $A_1,...,A_m$ are linearly independent if and only if
        ker(A) = \{0\}.
    \end{wtheorem}

    \begin{proof}
        Note 
        Ax =
        \footnotesize
        $\begin{bmatrix}
            A_1 & ... & A_m
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\
            \vdots \\
            x_m
        \end{bmatrix}$
        \normalsize
        = $x_1A_1 + ... + x_mA_m$
        = 0.
        Let $A_1,...,A_m$ be linearly independent.

        Then, the only solution to $x_1A_1 + ... + x_mA_m$ = 0
        is $(x_1,...,x_n)$ = 0 so ker(A) = \{0\}.

        \vspace{0.2cm}

        Suppose ker(A) = \{0\}.
        Then, the only solution
        to $x_1A_1 + ... + x_mA_m$ = Ax = 0 is x = 0.
        By {\color{red} theorem 2.1.5},
        $A_1,...,A_m$ are linearly independent.
    \end{proof}

    \vspace{0.5cm}



    \begin{example}
        Let T: $\mathbb{R}^3$ $\rightarrow$ $\mathbb{R}^4$ be
        T(x) = Ax where A =
        \footnotesize
        $\begin{bmatrix}
            1 & 1 & 1 \\
            1 & 2 & 4 \\
            1 & 3 & 7 \\
            1 & 4 & 10
        \end{bmatrix}$
        \normalsize.

        Find the im(A) and ker(A) and determine if the
        columns of A are independent.
    \end{example}

    \begin{tbox}
        im(A) = span(
        \scriptsize
        $\begin{bmatrix}
            1 \\
            1 \\
            1 \\
            1
        \end{bmatrix},
        \begin{bmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{bmatrix},
        \begin{bmatrix}
            1 \\
            4 \\
            7 \\
            10
        \end{bmatrix}$
        \normalsize)
        =
        \scriptsize
        $c_1\begin{bmatrix}
            1 \\
            1 \\
            1 \\
            1
        \end{bmatrix} +
        c_2\begin{bmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{bmatrix} +
        c_3\begin{bmatrix}
            1 \\
            4 \\
            7 \\
            10
        \end{bmatrix}$
        \normalsize
        for $c_1,c_2,c_3$ $\in$ $\mathbb{R}$

        \hspace{0.5cm}
        \scriptsize
        $\begin{bmatrix}
            1 & 1 & 1 & | & 0 \\
            1 & 2 & 4 & | & 0 \\
            1 & 3 & 7 & | & 0 \\
            1 & 4 & 10 & | & 0
        \end{bmatrix}$
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 1 & 1 & | & 0 \\
            0 & 1 & 3 & | & 0 \\
            0 & 2 & 6 & | & 0 \\
            0 & 3 & 9 & | & 0
        \end{bmatrix}$
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 0 & -2 & | & 0 \\
            0 & 1 & 3 & | & 0 \\
            0 & 0 & 0 & | & 0 \\
            0 & 0 & 0 & | & 0
        \end{bmatrix}$
        $\Leftrightarrow$
        $\begin{matrix*}[l]
            x_1 - 2x_3 = 0 \\
            x_2 + 3x_3 = 0
        \end{matrix*}$
        $\Rightarrow$
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3
        \end{bmatrix}$ =
        $\begin{bmatrix}
            2x_3 \\
            -3x_3 \\
            x_3
        \end{bmatrix}$
        \normalsize

        ker(A) =
        \scriptsize
        $x_3\begin{bmatrix}
            2 \\
            -3 \\
            1
        \end{bmatrix}$
        \normalsize
        $\not =$ \{0\}
        for $x_3$ $\in$ $\mathbb{R}$ so
        \scriptsize
        $\begin{bmatrix}
            1 \\
            1 \\
            1 \\
            1
        \end{bmatrix},
        \begin{bmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{bmatrix},
        \begin{bmatrix}
            1 \\
            4 \\
            7 \\
            10
        \end{bmatrix}$
        \normalsize
        are not linearly independent.
    \end{tbox}

    \vspace{0.5cm}





\subsection{ Basis \& Dimension }

    \begin{definition}{Basis \& Dimension}{16cm}
        Let set V $\subset$ $\mathbb{R}^n$ be subspace of $\mathbb{R}^n$.
        If $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ are linearly independent and span V
        (i.e. span($v_1,...,v_k$) = V), then $v_1,...,v_k$ form
        a {\color{lblue} basis} for V.

        The {\color{lblue} dimension} of V, dim(V), is the number of vectors in
        a basis of V.

        Since $e_1,...,e_n$, are linearly independent
        and any x $\in$ $\mathbb{R}^n$, is x = $x_1e_1 + ... + x_ne_n$
        so span($e_1,...,e_n$) = $\mathbb{R}^n$, then dim($\mathbb{R}^n$) = n.
        $e_1,...,e_n$ are called the {\color{lblue} standard basis vectors}.
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{\# Linearly independent vectors in V $\leq$
    \# vectors that span V}{16cm}
        Let $v_1,...,v_m$ $\in$ V be linearly independent
        and $u_1,...,u_k$ $\in$ V span V, then m $\leq$ k
    \end{wtheorem}

    \begin{proof}
        Let A =
        $\begin{bmatrix}
            v_1 & ... & v_m
        \end{bmatrix}$ $\in$ $M_{n \times m}(\mathbb{R})$
        and B =
        $\begin{bmatrix}
            u_1 & ... & u_k
        \end{bmatrix}$ $\in$ $M_{n \times k}(\mathbb{R})$.
        Since im(B) = span(B) = V, then for $v_1,...,v_m$ $\in$ V, there are
        $c_1,...,c_m$ $\in$ $\mathbb{R}^k$ such that $v_i$ = $Bc_i$:

        \hspace{0.5cm}
        A =
        $\begin{bmatrix}
            v_1 & ... & v_m
        \end{bmatrix}$ =
        $\begin{bmatrix}
            Bc_1 & ... & Bc_m
        \end{bmatrix}$ =
        B$\begin{bmatrix}
            c_1 & ... & c_m
        \end{bmatrix}$
        = BC
        \hspace{0.5cm}
        where C $\in$ $M_{k \times m}(\mathbb{R})$

        Thus, if Cx = 0, then Ax = BCx = B0 = 0 so ker(C) $\subset$ ker(A).
        Since $v_1,...,v_m$ $\in$ V be linearly independent, then by
        {\color{red} theorem 2.2.7}, ker(A) = \{0\} so ker(C) = \{0\}.
        Since Cx = 0 has a unique solution x = 0, then by
        {\color{orange} corollary 1.3.8}, k $\geq$ m.
    \end{proof}

    \newpage



    \begin{corollary}{The Dimension is unique}{16cm}
        Suppose $v_1,...,v_m$ $\in$ V and
        $u_1,...,u_k$ $\in$ V are bases for V, then dim(V) = m = k
    \end{corollary}

    \begin{proof}
        Since $v_1,...,v_m$ and $u_1,...,u_k$ are bases,
        then $v_1,...,v_m$ and $u_1,...,u_k$ are linearly independent
        and span V.
        By {\color{red} theorem 2.3.2}, m $\leq$ k and k $\leq$ m
        so m = k.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Linear combinations of a Basis are unique}{16cm}
        Let $v_1,...,v_k$ $\in$ $\mathbb{R}^n$ form a basis for
        V $\subset$ $\mathbb{R}^n$.
        
        Then for any v $\in$ V, there are unique $(c_1,...,c_k)$ such that:

        \hspace{0.5cm}
        v = $c_1v_1 + ... + c_kv_k$
    \end{wtheorem}

    \begin{proof}
        Since $v_1,...,v_k$ form a basis for V,
        then V = span($v_1,...,v_k$).
        Then for any v $\in$ V, then v $\in$ span($v_1,...,v_k$).
        Thus, there are $c_1,...,c_k$ $\in$ $\mathbb{R}$
        such that v = $c_1v_1 + ... + c_kv_k$.

        Let $a_1,...,a_k$ $\in$ $\mathbb{R}$ such that
        v = $a_1v_1 + ... + a_kv_k$. Then:

        \hspace{0.5cm}
        0 = $(c_1-a_1)v_1 + ... + (c_k-a_k)v_k$

        Since $v_1,...,v_k$ form a basis for V,
        then $v_1,...,v_k$ are linearly independent.
        Thus, by {\color{red} theorem 2.1.5}, $c_i - a_i$ = 0 for
        i = \{1,...,k\} so $c_i$ = $a_i$ for i = \{1,...,k\}.
        Thus, $(c_1,...,c_k)$ must be unique.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Connection between Span, Linear independence, and Basis}{16cm}
        Let V $\subset$ $\mathbb{R}^n$ where dim(V) = m. Then, m $\leq$ n and:
    \end{wtheorem}

    \begin{enumerate}[label=(\alph*), leftmargin=2cm, itemsep=0.1cm]
        \item If $u_1,...,u_k$ $\in$ V are linearly independent,
            then k $\leq$ m

        \item For $u_1,...,u_k$ $\in$ V, if span($u_1,...,u_k$) = V,
            then k $\geq$ m

        \item $u_1,...,u_m$ $\in$ V are linearly independent if and only if
            span($u_1,...,u_m$) = V
    \end{enumerate}

    \begin{proof}
        Since dim(V) = m, then there are $v_1,...,v_m$ $\in$ $\mathbb{R}^n$
        that are linearly independent and span V.
        Then for any v $\in$ V, there are $c_1,...,c_m$ where:

        \hspace{0.5cm}
        v = $c_1v_1 + ... + c_mv_m$
        \hspace{0.2cm}
        $\Leftrightarrow$
        \hspace{0.2cm}
        \footnotesize
        $\begin{bmatrix}
            v_1 & ... & v_m
        \end{bmatrix}
        \begin{bmatrix}
            c_1 \\
            \vdots \\
            c_m
        \end{bmatrix}$
        \normalsize
        = v
        \hspace{0.2cm}
        $\Leftrightarrow$
        \hspace{0.2cm}
        Ac = v
        \hspace{0.5cm}
        where A $\in$ $M_{n \times m}(\mathbb{R})$

        By {\color{red} theorem 2.3.4}, c is unique.
        Since A $\in$ $M_{n \times m}(\mathbb{R})$,
        then by {\color{orange} corollary 1.3.8}, n $\geq$ m.

        \rule[0.1cm]{16.7cm}{0.01cm}

        Let $u_1,...,u_k$ be linearly independent.
        Since $v_1,...,v_m$ span V, then
        by {\color{red} theorem 2.3.2}, k $\leq$ m.

        \rule[0.1cm]{16.7cm}{0.01cm}

        Let $u_1,...,u_k$ span V.
        Since $v_1,...,v_m$ are linearly independent, then
        by {\color{red} theorem 2.3.2}, m $\leq$ k
        
        \rule[0.1cm]{16.7cm}{0.01cm}

        Suppose $u_1,...,u_m$ $\in$ V are linearly independent.
        Let v $\in$ V.
        If v $\not \in$ span($u_1,...,u_m$), then
        by {\color{red} theorem 2.1.6}, $u_1,...,u_m$,v are linearly independent
        contradicting that m+1 = dim(V) = m.
        Thus, any v $\in$ V is v $\in$ span($u_1,...,u_m$) so
        V $\subset$ span($u_1,...,u_m$). Since span($u_1,...,u_m$) $\subset$ V,
        then span($u_1,...,u_m$) = V.

        \vspace{0.2cm}

        Suppose span($u_1,...,u_m$) = V.
        Suppose there are $c_1,...,c_m$ where not all $c_i$ = 0 such that:

        \hspace{0.5cm}
        $c_1u_1 + ... + c_mu_m$ = 0

        If there is one $c_i$ $\not =$ 0, then
        $c_iu_i$ = $c_1u_1 + ... + c_mu_m$ = 0 implies $u_i$ = 0
        which is a dependent vector. Thus, there are at least two
        $c_i$ $\not =$ 0. Then, $u_i$ =
        $\frac{-c_1}{c_i}u_1 + ... + \frac{-c_{i-1}}{c_i}u_{i-1}
        + \frac{-c_{i+1}}{c_i}u_{i+1} + ... + \frac{-c_m}{c_i}u_m$
        where at least one $c_{j \not = i}$ $\not =$ 0
        so $u_i$ is a dependent vector. So there are less than m
        independent vectors contradicting that m = dim(V) $<$ m.
        Thus, all $c_i$ = 0 so $u_1,...,u_m$ are independent.
    \end{proof}

    \newpage



    \begin{wtheorem}{Determining Basis for Im(A) using rref}{16cm}
        Let A =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}$ $\in$ $M_{n \times k}(\mathbb{R})$
        where each $v_i$ $\in$ $\mathbb{R}^n$.
        Then the $v_i$ that are linearly independent are the columns in rref(A)
        with pivots.
        Thus, such $v_i$ form a basis for im(A).
    \end{wtheorem}

    \begin{proof}
        By {\color{red} theorem 2.2.5}, im(A) = span(A) = span($v_1,...,v_k$).
        Suppose the i-th column of rref($[A \ | \ 0]$) does not contain a pivot.
        Then, Ax = 0 where x $\in$ $\mathbb{R}^k$ has a free variable at $x_i$
        so ker(A) has nonzero solutions.
        Thus, by {\color{red} theorem 2.2.7}, $v_1,...,v_k$
        are linearly dependent.
        But, if all columns without pivots are removed are removed from rref(A)
        to make B, then Bx = 0 has only pivots and thus, by
        {\color{red} theorem 1.3.7}, there is a unique solution.
        Since B0 = 0, then ker(B) = \{0\}
        and thus, the columns with pivots are linearly independent
        and by {\color{red} theorem 2.1.4}, the span(B) = span(A) = im(A)
        so the columns with pivots form a basis for im(A).
    \end{proof}

    \vspace{0.5cm}



    \begin{corollary}{dim(im(A)) = rank(A)}{16cm}
        For any A $\in$ $M_{m \times n}(\mathbb{R})$, then:

        \hspace{0.5cm}
        dim(im(A)) = rank(A)
    \end{corollary}

    \begin{proof}
        Let A =
        $\begin{bmatrix}
            A_1 & ... & A_n 
        \end{bmatrix}$ where $A_i$ $\in$ $\mathbb{R}^m$.
        By {\color{red} theorem 2.3.6}, the dim(im(A)),
        the number of vectors in a basis for im(A) is the
        same as the number of pivots in rref(A), rank(A).
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Rank-Nullity Theorem}{16cm}
        For any A $\in$ $M_{m \times n}(\mathbb{R})$,
        the dim(ker(A)) is called the {\color{lblue} nullity} of A where:

        \hspace{0.5cm}
        dim(im(A)) + dim(im(A)) = n
    \end{wtheorem}

    \begin{proof}
        Note the number of free variables + the number of pivot variables = n.

        By {\color{orange} corollary 2.3.7},
        the number of pivot variables, rank(A) = dim(im(A)).
        
        If the i-th column doesn't have a pivot, then the solutions to Ax = 0
        are linear combinations of vectors $v_i$ with 1 in the i-th row
        and 0 in any j-th row where the j-th column doesn't have a pivot
        so each $v_i$ is linearly independent and span ker(A).
        Thus, the number of free variables is equal to
        the number of $v_i$, dim(ker(A)), so dim(im(A)) + dim(im(A)) = n.
    \end{proof}

    \vspace{0.5cm}



    \begin{example}
        Let T: $\mathbb{R}^5$ $\rightarrow$ $\mathbb{R}^2$
        be T(x) = Ax where A =
        \scriptsize
        $\begin{bmatrix}
            1 & 2 & 3 & 3 & 0 \\
            4 & 5 & 6 & 9 & 6
        \end{bmatrix}$
        \normalsize.

        Find a basis for the im(A) and ker(A).
        Verify dim(im(A)) + dim(ker(A)) = 5.
    \end{example}

    \begin{tbox}
        \scriptsize
        $\begin{bmatrix}
            1 & 2 & 3 & 3 & 0 & | & 0 \\
            4 & 5 & 6 & 9 & 6 & | & 0
        \end{bmatrix}$
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 2 & 3 & 3 & 0 & | & 0 \\
            0 & -3 & -6 & -3 & 6 & | & 0
        \end{bmatrix}$
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 2 & 3 & 3 & 0 & | & 0 \\
            0 & 1 & 2 & 1 & -2 & | & 0
        \end{bmatrix}$
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 0 & -1 & 1 & 4 & | & 0 \\
            0 & 1 & 2 & 1 & -2 & | & 0
        \end{bmatrix}$
        \normalsize

        \hspace{0.5cm}
        im(A) = span(
        \scriptsize
        $\begin{bmatrix}
            1 \\
            4 
        \end{bmatrix},
        \begin{bmatrix}
            2 \\
            5 
        \end{bmatrix}$
        \normalsize) =
        \scriptsize
        $c_1\begin{bmatrix}
            1 \\
            4 
        \end{bmatrix} +
        c_2\begin{bmatrix}
            2 \\
            5 
        \end{bmatrix}$
        \normalsize
        for $c_1,c_2$ $\in$ $\mathbb{R}$

        \scriptsize
        $\begin{bmatrix}
            1 & 0 & -1 & 1 & 4 & | & 0 \\
            0 & 1 & 2 & 1 & -2 & | & 0
        \end{bmatrix}$
        $\Leftrightarrow$
        $\begin{matrix*}[l]
            x_1 - x_3 + x_4 + 4x_5 = 0 \\
            x_2 + 2x_3 + x_4 - 2x_5 = 0
        \end{matrix*}$
        $\Rightarrow$
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3 \\
            x_4 \\
            x_5
        \end{bmatrix}$ =
        $\begin{bmatrix}
            x_3 - x_4 - 4x_5 \\
            -2x_3 - x_4 + 2x_5 \\
            x_3 \\
            x_4
            x_5 
        \end{bmatrix}$ =
        $x_3\begin{bmatrix}
            1 \\
            -2 \\
            1 \\
            0 \\
            0
        \end{bmatrix} +
        x_4\begin{bmatrix}
            -1 \\
            -1 \\
            0 \\
            1 \\
            0
        \end{bmatrix}  +
        x_5\begin{bmatrix}
            -4 \\
            2 \\
            0 \\
            0 \\
            1
        \end{bmatrix}$
        \normalsize

        \hspace{0.5cm}
        ker(A) = span(
        \scriptsize
        $\begin{bmatrix}
            1 \\
            -2 \\
            1 \\
            0 \\
            0
        \end{bmatrix},
        \begin{bmatrix}
            -1 \\
            -1 \\
            0 \\
            1 \\
            0
        \end{bmatrix},
        \begin{bmatrix}
            -4 \\
            2 \\
            0 \\
            0 \\
            1
        \end{bmatrix}$
        \normalsize) =
        \scriptsize
        $x_3\begin{bmatrix}
            1 \\
            -2 \\
            1 \\
            0 \\
            0
        \end{bmatrix} +
        x_4\begin{bmatrix}
            -1 \\
            -1 \\
            0 \\
            1 \\
            0
        \end{bmatrix}  +
        x_5\begin{bmatrix}
            -4 \\
            2 \\
            0 \\
            0 \\
            1
        \end{bmatrix}$
        \normalsize
        for $x_3,x_4$ $\in$ $\mathbb{R}$

        Then, dim(im(A)) + dim(ker(A))
        = 2 + 3 = 5
    \end{tbox}

    \newpage





\subsection{ Injectivity \& Surjectivity }

    \begin{definition}{Injectivity and Surjectivity}{16cm}
        Let T: V $\rightarrow$ W be a linear transformation.

        T is {\color{lblue} injective} if for any w $\in$ T(V) = im(T),
        there is a unique v $\in$ V such that T(v) = w

        T is {\color{lblue} surjective} if for any w $\in$ W,
        there is a v $\in$ V such that T(v) = w
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Connection between Invertibility, Injectivity,
    and Surjectivity }{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation where T(x) = Ax.
        
        Then, T is invertible if and only if T is injective and surjective.
    \end{wtheorem}

    \begin{proof}
        Suppose T is invertible.
        Let y $\in$ $\mathbb{R}^n$.
        Then, by {\color{red} theorem 1.5.12},
        there is a unique x $\in$ $\mathbb{R}^n$ such that Ax = T(x) = y.
        Thus, T is injective and surjective.

        \vspace{0.2cm}

        Suppose T is injective and surjective.
        Since T is surjective, then for any y $\in$ $\mathbb{R}^n$,
        there is a x $\in$ $\mathbb{R}^n$ such that Ax = T(x) = y.
        Since T is injective, then x is unique.
        Then, by {\color{red} theorem 1.5.12}, T is invertible.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Connection between Invertibility, Span, and
    Linear independence}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation T(x) = Ax where A =
        $\begin{bmatrix}
            A_1 & ... & A_n
        \end{bmatrix}$.

        Then, T is invertible if and only if $A_1,...,A_n$
        are linearly independent and span $\mathbb{R}^n$.
    \end{wtheorem}

    \begin{proof}
        Suppose T is invertible.
        By {\color{red} theorem 1.5.12}, for any y $\in$ $\mathbb{R}^n$,
        there is a unique x $\in$ $\mathbb{R}^n$ where
        Ax = y. Thus, span($A_1,...,A_n$) = im(A).
        Since dim($\mathbb{R}^n$) = n, then $A_1,...,A_n$ span $\mathbb{R}^n$,
        then by {\color{red} theorem 2.3.5},
        $A_1,...,A_n$ are linearly independent.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Injectivity $\Leftrightarrow$ Surjectivity}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation where T(x) = Ax
        where A =
        $\begin{bmatrix}
            A_1 & ... & A_n
        \end{bmatrix}$.

        Then, T is surjective if and only if T is injective.
    \end{wtheorem}

    \begin{proof}
        Suppose T is surjective.
        Then for any y $\in$ $\mathbb{R}^n$,
        there is a x $\in$ $\mathbb{R}^n$ where
        Ax = T(x) = y.
        Thus, span($A_1,...,A_n$) = im(A) = $\mathbb{R}^n$.
        Since dim($\mathbb{R}^n$) = n, then by {\color{red} theorem 2.3.5},
        $A_1,...,A_n$ are linearly independent.
        By {\color{red} theorem 2.4.3}, T is invertible,
        then by {\color{red} theorem 2.4.2}, T is injective.

        \vspace{0.2cm}

        Suppose T is injective.
        Then for any y $\in$ im(A),
        there is a unique x $\in$ $\mathbb{R}^n$ where
        Ax = T(x) = y.
        Suppose there is a linearly dependent $A_i$.
        Then there are $c_1,...,c_{i-1},c_{i+1},c_n$ where
        at least one $c_i$ $\not =$ 0 such that
        $A_i$ = $c_1A_1 + ... + c_{i-1}A_{i-1} + c_{i+1}A_{i+1} + ... + c_nA_n$.
        Then:

        \hspace{0.5cm}
        Ax
        = $x_1A_1 + ... + x_nA_n$

        \hspace{1.15cm}
        = $(x_1 + x_ic_1)A_1 + ... + (x_{i-1} + x_ic_{i-1})A_{i-1}
            + (x_{i+1} + x_ic_{i+1})A_{i+1} + ... + (x_n + x_ic_n)A_n$

        If $x_i$ $\not =$ 0, then x = $(x_1,...,x_n)$
        is not the only solution to y = Ax contradicting that x is unique.
        Thus, $x_i$ = 0. Similarily, if any other $A_j$ is linearly dependent,
        then $x_j$ = 0. Thus, Ax = $x_1A_1 + ... + x_nA_n$
        = $x_{i_1}A_{i_1} + ... + x_{i_k}A_{i_k}$
        where $A_{i_1},...,A_{i_k}$ are the $A_i$ that are linearly independent.
        Thus, for Ax = 0, then each $x_{i_1}$ = 0.
        Since then all $x_i$ = 0, then no $A_i$ is linearly dependent
        so $A_1,...,A_n$ are linearly independent.
        By {\color{red} theorem 2.4.3}, T is invertible,
        then by {\color{red} theorem 2.4.2}, T is surjective.
    \end{proof}

    \newpage



    \begin{wtheorem}{Invertibility Equivalences Extended}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation where T(x) = Ax
        where A =
        $\begin{bmatrix}
            A_1 & ... & A_n
        \end{bmatrix}$.
        Then the following are equivalent:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item A is invertible
            
            \item rref(A) = $I_{n \times n}$
            
            \item rank(A) = n
            
            \item For any y $\in$ $\mathbb{R}^n$, then Ax = y
                has a unique solution x

            \item Ax = 0 has only the trivial soluton x = 0
                \hspace{0.5cm}
                ( i.e ker(A) = \{0\} )
        
            \item $A_1,...,A_n$ are linearly independent
            
            \item im(A) = span($A_1,...,A_n$) = $\mathbb{R}^n$
            
            \item T is injective
            
            \item T is surjective
        \end{enumerate}
    \end{wtheorem}

    \begin{proof}
        \footnotesize
        (a) $\Leftrightarrow$
        $\begin{cases}
            (b \rightarrow e) & {\color{red} \text{theorem 1.5.12}} \\
            (f) & {\color{red} \text{theorem 2.4.3 and 2.3.5}}. \\
                & \text{(a) $\Rightarrow$ (f) only needs 2.4.3,
                    but (a) $\Leftarrow$ (f) needs 2.4.3, 2.3.5} \\
            (g) & {\color{red} \text{theorem 2.4.3 and 2.3.5}} \\
                & \text{(a) $\Rightarrow$ (g) only needs 2.4.3,
                    but (a) $\Leftarrow$ (g) needs 2.4.3, 2.3.5} \\
            (h) & {\color{red} \text{theorem 2.4.2 and 2.4.4}} \\
                & \text{(a) $\Rightarrow$ (h) only needs 2.4.2,
                    but (a) $\Leftarrow$ (h) needs 2.4.2, 2.4.4} \\
            (i) & {\color{red} \text{theorem 2.4.2 and 2.4.4}} \\
                & \text{(a) $\Rightarrow$ (i) only needs 2.4.2,
                    but (a) $\Leftarrow$ (i) needs 2.4.2, 2.4.4} \\
        \end{cases}$
        \normalsize
    \end{proof}

    \vspace{0.5cm}



    \begin{example}
        Let T: $\mathbb{R}^4$ $\rightarrow$ $\mathbb{R}^4$
        be T(x) = Ax where A =
        \scriptsize
        $\begin{bmatrix}
            1 & 0 & 2 & 4 \\
            0 & 1 & -3 & -1 \\
            3 & 4 & -6 & 8 \\
            0 & -1 & 3 & 1
        \end{bmatrix}$.
        \normalsize

        Find if T is invertible, im(T), dim(im(T)), ker(T), and dim(ker(T)).
    \end{example}

    \begin{tbox}
        $[A \ | \ I_{4 \times 4}]$ =
        \scriptsize
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & 1 & 0 & 0 & 0 \\
            0 & 1 & -3 & -1 & | & 0 & 1 & 0 & 0 \\
            3 & 4 & -6 & 8 & | & 0 & 0 & 1 & 0 \\
            0 & -1 & 3 & 1 & | & 0 & 0 & 0 & 1
        \end{bmatrix}$
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & 1 & 0 & 0 & 0 \\
            0 & 1 & -3 & -1 & | & 0 & 1 & 0 & 0 \\
            0 & 4 & -12 & -4 & | & -3 & 0 & 1 & 0 \\
            0 & -1 & 3 & 1 & | & 0 & 0 & 0 & 1
        \end{bmatrix}$

        \hspace{1.8cm}
        $\Rightarrow$
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & 1 & 0 & 0 & 0 \\
            0 & 1 & -3 & -1 & | & 0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0 & | & -3 & -4 & 1 & 0 \\
            0 & 0 & 0 & 0 & | & 0 & 1 & 0 & 1
        \end{bmatrix}$
        \normalsize
        $\Rightarrow$
        rref(A) $\not =$ $I_{4 \times 4}$
        $\Rightarrow$
        A is not invertible

        \vspace{0.3cm}

        Since the 1st and 2nd column have pivots,
        then (1,0,3,0) and (0,1,4,-1) form a basis for im(T).

        \hspace{0.5cm}
        im(T) = $c_1(1,0,3,0) + c_2(0,1,4,-1)$
        \hspace{0.5cm}
        for $c_1,c_2$ $\in$ $\mathbb{R}$

        \hspace{0.5cm}
        where (1,0,3,0), (0,1,4,-1) form a basis for im(T) so dim(im(T)) = 2

        \vspace{0.3cm}

        To find ker(T), (i.e. solving Ax = 0),
        replace the right matrix by a column with all 0 entries:

        \hspace{0.5cm}
        \scriptsize
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & 0 \\
            0 & 1 & -3 & -1 & | & 0 \\
            0 & 0 & 0 & 0 & | & 0 \\
            0 & 0 & 0 & 0 & | & 0
        \end{bmatrix}$
        \hspace{0.2cm}
        $\Leftrightarrow$
        \hspace{0.2cm}
        $\begin{matrix*}
            x_1 + 2x_3 + 4x_4 = 0 \\
            x_2 - 3x_3 - x_4 = 0
        \end{matrix*}$
        \hspace{0.2cm}
        $\Leftrightarrow$
        \hspace{0.2cm}
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3 \\
            x_4
        \end{bmatrix}$ =
        $\begin{bmatrix}
            -2x_3 - 4x_4 \\
            3x_3 + x_4 \\
            x_3 \\
            x_4 
        \end{bmatrix}$ =
        $x_3\begin{bmatrix}
                -2 \\
                3 \\
                1 \\
                0    
            \end{bmatrix} +
        x_4\begin{bmatrix}
                -4 \\
                1 \\
                0 \\
                1    
            \end{bmatrix}$
        
        \normalsize
        \hspace{0.5cm}
        ker(T) =
        = $x_3(-2,3,1,0) + x_4(-4,1,0,1)$
        \hspace{0.5cm}
        for $x_3,x_4$ $\in$ $\mathbb{R}$
        
        \hspace{0.5cm}
        where (-2,3,1,0), (-4,1,0,1) form a basis for ker(T) so dim(ker(T)) = 2
    \end{tbox}

    \newpage





\subsection{ Coordinates }

    \begin{definition}{Coordinates}{16cm}
        For V $\subset$ $\mathbb{R}^n$, let $v_1,...,v_k$ $\in$ V be
        a basis for V.
        Then the {\color{lblue} coordinates} of v $\in$ V relative to basis 
        $\mathcal{B}$ = $v_1,...,v_k$ are $c_1,...,c_k$ $\in$ $\mathbb{R}$
        such that:

        \hspace{0.5cm}
        v = $c_1v_1 + ... + c_kv_k$

        Then the {\color{lblue} coordinate vector}
        $[v]_{\mathcal{B}}$ = $(c_1,...,c_k)$ $\in$ $\mathbb{R}^k$.
        Let B =
        $\begin{bmatrix}
            v_1 & ... & v_k
        \end{bmatrix}$ $\in$ $M_{n \times k}(\mathbb{R})$.

        \hspace{0.5cm}
        v = $c_1v_1 + ... + c_kv_k$
        = B$[v]_{\mathcal{B}}$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Properties of Coordinates}{16cm}
        For V $\subset$ $\mathbb{R}^n$, let $\mathcal{B}$ = $v_1,...,v_k$ $\in$ V
        be a basis for V. For x,y $\in$ V and c $\in$ $\mathbb{R}$:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item $[x+y]_{\mathcal{B}}$ = $[x]_{\mathcal{B}} + [y]_{\mathcal{B}}$
            
            \item $[cx]_{\mathcal{B}}$ = $c[x]_{\mathcal{B}}$
        \end{enumerate}
    \end{wtheorem}

    \begin{proof}
        Let x = $a_1v_1 + ... + a_kv_k$
        and y = $b_1v_1 + ... + b_kv_k$
        for $a_1,...,a_k,b_1,...,b_k$ $\in$ $\mathbb{R}$.
        Let $c_1,c_2$ $\in$ $\mathbb{R}$.

        \hspace{0.5cm}
        $[c_1x+c_2y]_{\mathcal{B}}$
        = $(c_1a_1+c_2b_1,...,c_1a_k+c_2b_k)$
        = $c_1(a_1,...,c_1a_k) + c_2(b_1,...,b_k)$
        = $c_1[x]_{\mathcal{B}} + c_2[y]_{\mathcal{B}}$
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Linear transformation by Coordinates}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation where T(x) = Ax.
        Let $\mathcal{B}$ = $v_1,...,v_n$
        be a basis for $\mathbb{R}^n$.
        Then for any x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        $[T(x)]_{\mathcal{B}}$
        = $[Ax]_{\mathcal{B}}$
        = $A_{\mathcal{B}}[x]_{\mathcal{B}}$

        where the {\color{lblue} $\mathcal{B}$-matrix of T}, $A_{\mathcal{B}}$ =
        $\begin{bmatrix}
            [A(v_1)]_{\mathcal{B}} & ... & [A(v_n)_{\mathcal{B}}]
        \end{bmatrix}$ $\in$ $M_{n \times n}(\mathbb{R})$
    \end{wtheorem}

    \begin{proof}
        Let x = $c_1v_1 + ... + c_nv_n$ for $c_1,...,c_n$ $\in$ $\mathbb{R}$.
        Then $[x]_{\mathcal{B}}$ = $(c_1,...,c_n)$:

        \hspace{0.5cm}
        $[T(x)]_{\mathcal{B}}$
        = $[c_1A(v_1) + ... + c_nA(v_n)]_{\mathcal{B}}$
        = $c_1[A(v_1)]_{\mathcal{B}} + ... + c_n[A(v_n)]_{\mathcal{B}}$
        
        \hspace{1.9cm}
        =
        $\begin{bmatrix}
            [A(v_1)]_{\mathcal{B}} & ... & [A(v_n)_{\mathcal{B}}]
        \end{bmatrix}
        \begin{bmatrix}
            c_1 \\
            \vdots \\
            c_n
        \end{bmatrix}$ =
        $\begin{bmatrix}
            [A(v_1)]_{\mathcal{B}} & ... & [A(v_n)_{\mathcal{B}}]
        \end{bmatrix}
        [x]_{\mathcal{B}}$  
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Relationship between Ax and
    $A_{\mathcal{B}}x_{\mathcal{B}}$}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation where T(x) = Ax.
        Let $\mathcal{B}$ = $v_1,...,v_n$ be a basis for $\mathbb{R}^n$
        where $[T(x)]_{\mathcal{B}}$ = $A_{\mathcal{B}}[x]_{\mathcal{B}}$.
        Then:

        \hspace{0.5cm}
        AB = B$A_{\mathcal{B}}$
        \hspace{1cm}
        $A_{\mathcal{B}}$ = $B^{-1}$AB

        where B =
        $\begin{bmatrix}
            v_1 & ... & v_n
        \end{bmatrix}$ $\in$ $M_{n \times n}(\mathbb{R})$
        is invertible
        and $[x]_{\mathcal{B}}$ = $B^{-1}$x
    \end{wtheorem}

    \begin{proof}
        Let B =
        $\begin{bmatrix}
            v_1 & ... & v_n
        \end{bmatrix}$.
        Since x = B$[x]_{\mathcal{B}}$ and T(x) = B$[T(x)]_{\mathcal{B}}$, then:

        \hspace{0.5cm}
        T(x) = Ax
        = AB$[x]_{\mathcal{B}}$
        \hspace{1cm}
        T(x) = B$[T(x)]_{\mathcal{B}}$
        = B$A_{\mathcal{B}}[x]_{\mathcal{B}}$

        Thus, AB = B$A_{\mathcal{B}}$.
        Since $v_1,...,v_n$ is a basis, then
        by {\color{red} theorem 2.4.5}, B is invertible.

        Since x = B$[x]_{\mathcal{B}}$, then $[x]_{\mathcal{B}}$ = $B^{-1}$x
    \end{proof}

    \newpage



    \begin{corollary}{Change of Bases}{16cm}
        Let T: $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        be a linear transformation.
        Let $\mathcal{B}$ = $v_1,...,v_n$ be a basis for $\mathbb{R}^n$
        where $[T(x)]_{\mathcal{B}}$ = $A_{\mathcal{B}}[x]_{\mathcal{B}}$
        and $\mathcal{B}'$ = $v_1',...,v_n'$ also be a basis for $\mathbb{R}^n$
        where $[T(x)]_{\mathcal{B}'}$ = $A_{\mathcal{B}'}[x]_{\mathcal{B}'}$.

        \hspace{0.5cm}
        $A_{\mathcal{B}'}$
        = $S^{-1} A_{\mathcal{B}}S$

        where S =
        $\begin{bmatrix}
            v_1 & ... & v_n
        \end{bmatrix}^{-1}
        \begin{bmatrix}
            v_1' & ... & v_n'
        \end{bmatrix}$
        is invertible
        and $[x]_{\mathcal{B}'}$ = $S^{-1}[x]_{\mathcal{B}}$ 
    \end{corollary}

    \begin{proof}
        Let T(x) = Ax. By {\color{red} theorem 2.5.4}, for
        invertible B =
        $\begin{bmatrix}
            v_1 & ... & v_n
        \end{bmatrix}$ and B' =
        $\begin{bmatrix}
            v_1' & ... & v_n'
        \end{bmatrix}$:

        \hspace{0.5cm}
        A = B$A_{\mathcal{B}}B^{-1}$
        \hspace{1cm}
        A = B'$A_{\mathcal{B}'}(B')^{-1}$

        \hspace{0.5cm}
        B$A_{\mathcal{B}}B^{-1}$
        = B'$A_{\mathcal{B}'}(B')^{-1}$
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        $A_{\mathcal{B}'}$
        = $(B')^{-1}$B$A_{\mathcal{B}}B^{-1}B'$
        = $(B^{-1}B')^{-1} A_{\mathcal{B}} (B^{-1}B')$

        Since $v_1,...,v_n$ and $u_1,...,u_n$ are bases, then
        by {\color{red} theorem 1.5.13}, $B^{-1}B'$ is invertible.

        Since x = B$[x]_{\mathcal{B}}$ and x = B'$[x]_{\mathcal{B}'}$, then:
        
        \hspace{0.5cm}
        $[x]_{\mathcal{B}'}$
        = $(B')^{-1}$x
        = $(B')^{-1}$B$[x]_{\mathcal{B}}$
        = $(B^{-1}B')^{-1} [x]_{\mathcal{B}}$
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Matrix Similarity}{16cm}
        Let A,B $\in$ $M_{n \times n}(\mathbb{R})$.
        Then A is {\color{lblue} similar} to B if there is an invertible
        X $\in$ $M_{n \times n}(\mathbb{R})$ where:

        \hspace{0.5cm}
        AX = XB
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Properties of Matrix Similarity}{16cm}
        Let A,B,C $\in$ $M_{n \times n}(\mathbb{R})$.
    \end{wtheorem}

    \begin{enumerate}[label=(\alph*), leftmargin=2cm, itemsep=0.1cm]
        \item {\color{lgreen} Reflexivity}
        
            \hspace{0.5cm}
            A is similar to A

            \begin{proof}[15cm]
                A$I_{n \times n}$ = A = $I_{n \times n}$A
            \end{proof}

        \item {\color{lgreen} Symmetry}
        
            \hspace{0.5cm}
            If A is similar to B, then B is similar to A

            \begin{proof}[15cm]
                Since A is similar to B, there is an invertible
                X $\in$ $M_{n \times n}(\mathbb{R})$ where AX = XB.

                \hspace{0.5cm}
                B$X^{-1}$
                = $X^{-1}$XB$X^{-1}$
                = $X^{-1}$AX$X^{-1}$
                = $X^{-1}$A
            \end{proof}

        \item {\color{lgreen} Transitvity}
        
            \hspace{0.5cm}
            If A is similar to B and B is similar to C, then
            A is similar to C

            \begin{proof}[15cm]
                Since A is similar to B, there is an invertible
                $X_1$ $\in$ $M_{n \times n}(\mathbb{R})$ where A$X_1$ = $X_1$B.

                Since B is similar to C, there is an invertible
                $X_2$ $\in$ $M_{n \times n}(\mathbb{R})$ where B$X_2$ = $X_2$C.

                \hspace{0.5cm}
                A$X_1X_2$
                = $X_1$B$X_2$
                = $X_1X_2$C

                Since $X_1,X_2$ are invertible,
                then by {\color{red} theorem 1.5.13}, $X_1X_2$
                is invertible.
            \end{proof}
    \end{enumerate}

    \newpage



    \begin{example}
        Let T: $\mathbb{R}^3$ $\rightarrow$ $\mathbb{R}^3$
        be be T(x) = Ax =
        $\begin{bmatrix}
            -1 & 1 & 0 \\
            0 & -2 & 2 \\
            3 & -9 & 6
        \end{bmatrix}$x
        where $\mathcal{B}$ = (1,1,1), (1,2,3), and (1,3,6)
        is a basis for $\mathbb{R}^3$.
        Find $A_{\mathcal{B}}$.
        Find and verify $[T(-1,0,1)]_{\mathcal{B}}$.
    \end{example}

    \begin{tbox}
        $A_{\mathcal{B}}$ =
        $\begin{bmatrix}
            1 & 1 & 1 \\
            1 & 2 & 3 \\
            1 & 3 & 6
        \end{bmatrix}^{-1}
        \begin{bmatrix}
            -1 & 1 & 0 \\
            0 & -2 & 2 \\
            3 & -9 & 6
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1 & 1 \\
            1 & 2 & 3 \\
            1 & 3 & 6
        \end{bmatrix}$ =
        $\begin{bmatrix}
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 2
        \end{bmatrix}$

        $[(-1,0,1)]_{\mathcal{B}}$
        = $\begin{bmatrix}
            1 & 1 & 1 \\
            1 & 2 & 3 \\
            1 & 3 & 6
        \end{bmatrix}^{-1}
        \begin{bmatrix}
            -1 \\
            0 \\
            1
        \end{bmatrix}$ =
        $\begin{bmatrix}
            -2 \\
            1 \\
            0
        \end{bmatrix}$

        $[T(-1,0,1)]_{\mathcal{B}}$
        = $A_{\mathcal{B}} [(-1,0,1)]_{\mathcal{B}}$ =
        $\begin{bmatrix}
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 2
        \end{bmatrix}
        \begin{bmatrix}
            -2 \\
            1 \\
            0
        \end{bmatrix}$
        = (0,1,0)

        Since
        \footnotesize
        $0\begin{bmatrix}
            1 \\
            1 \\
            1
        \end{bmatrix} +
        1\begin{bmatrix}
            1 \\
            2 \\
            3
        \end{bmatrix} +
        0\begin{bmatrix}
            1 \\
            3 \\
            6
        \end{bmatrix}$
        \normalsize
        = (1,2,3)
        and T(-1,0,1) =
        \footnotesize
        $\begin{bmatrix}
            -1 & 1 & 0 \\
            0 & -2 & 2 \\
            3 & -9 & 6
        \end{bmatrix}
        \begin{bmatrix}
            -1 \\
            0 \\
            1
        \end{bmatrix}$
        \normalsize
        = (1,2,3), then it is true that
        $[T(-1,0,1)]_{\mathcal{B}}$ = (0,1,0).
    \end{tbox}




