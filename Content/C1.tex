\newpage

\section[Day 1: Vectors \& Matrices]{ Vectors \& Matrices }

\subsection{ Vectors }

    \begin{definition}{Vector}{16cm}
        A {\color{lblue} vector} x $\in$ $\mathbb{R}^n$:

        \hspace{0.5cm}
        x =
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            \hdots \\
            x_n
        \end{bmatrix}$
        = $(x_1,x_2,...,x_n)$
        \hspace{0.5cm}
        where each $x_i$ $\in$ $\mathbb{R}$

        Let 0 $\in$ $\mathbb{R}^n$ be 0 = $\underbrace{(0,...,0)}_n$.
        Then for any x,y $\in$ $\mathbb{R}^n$ and scalar c $\in$ $\mathbb{R}$,
        define:

        \hspace{0.5cm}
        x + y = $(x_1 + y_1 , x_2 + y_2 , ... , x_n + y_n)$
        \hspace{1cm}
        cx = $(cx_1 , cx_2 , ... , cx_n)$

        Also, define the {\color{lblue} length} of x:

        \hspace{0.5cm}
        $||x||$ = $|x|$ = $\sqrt{x_1^2 + x_2^2 + ... + x_n^2}$
        = $(\sum_{i=1}^n x_i^2)^{\frac{1}{2}}$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Properties of Vectors}{16cm}
        For any x,y,z $\in$ $\mathbb{R}^n$ and $c_1,c_2$ $\in$ $\mathbb{R}$:
    \end{wtheorem}

    \begin{enumerate}[label=(\alph*), leftmargin=2cm, itemsep=0.1cm]
        \item {\color{lgreen} Commutativity}
        
            \hspace{0.5cm}
            x + y = y + x

            \begin{proof}[15cm]
                x + y
                = ($x_1+y_1,x_2+y_2,...,x_n+y_n$)
                = ($y_1+x_1,y_2+x_2,...,y_n+x_n$)
                = y + x
            \end{proof}

        \item {\color{lgreen} Addititive Associativity}
        
            \hspace{0.5cm}
            (x + y) + z = x + (y + z)

            \begin{proof}[15cm]
                (x + y) + z
                = ($x_1+y_1,x_2+y_2,...,x_n+y_n$) + z

                \hspace{2.1cm}
                = ($x_1+y_1+z_1,x_2+y_2+z_2,...,x_n+y_n+z_n$)

                \hspace{2.1cm}
                = x + ($y_1+z_1,y_2+z_2,...,y_n+z_n$)
                = x + (y + z)
            \end{proof}

        \item {\color{lgreen} Additive Identity}
        
            There exists a unique $0_v$ $\in$ $\mathbb{R}^n$
            such that for all x $\in$ $\mathbb{R}^n$:

            \hspace{0.5cm}
            $0_v$ + x = x

            Moreover, $0_v$ = 0.
            This only holds for $\mathbb{R}^n$ and not all vector spaces.

            \begin{proof}[15cm]
                Suppose there are $0_{v_1},0_{v_2}$ where
                $0_{v_1}$ + x = x and $0_{v_2}$ + x = x for all x. Then:

                \hspace{0.5cm}
                $0_{v_1}$
                = $0_{v_2}$ + $0_{v_1}$
                = $0_{v_1}$ + $0_{v_2}$
                = $0_{v_2}$

                Thus, $0_v$ must be unique.

                Since
                0 + x
                = $(0+x_1,...,0+x_n)$
                = $(x_1,...,x_n)$
                = x, then $0_v$ = 0.
            \end{proof}

        \item {\color{lgreen} Additive Inverse}
        
            For any x, there exists a unique -x $\in$ $\mathbb{R}^n$ such that:

            \hspace{0.5cm}
            x + (-x) = $0_v$

            Moreover, $-x$ = $(-1)x$.
            This only holds for $\mathbb{R}^n$ and not all vector spaces.

            \begin{proof}[15cm]
                Suppose there are $(-x)_a,(-x)_b$ where
                x + $(-x)_a$ = $0_v$ and x + $(-x)_b$ = $0_v$. Then:

                \hspace{0.5cm}
                $(-x)_a$
                = $(-x)_a + 0_v$
                = $(-x)_a + x + (-x)_b$
                = $0_v + (-x)_b$
                = $(-x)_b$

                Thus, $-x$ must be unique.

                Since
                $(-1)x$ + x
                = $(-x_1+x_1,...,-x_n+x_n)$
                = 0,
                then $-x$ = $(-1)x$.
            \end{proof}

            \newpage

        \item {\color{lgreen} Distributivity}
        
            \hspace{0.5cm}
            $c_1(x+y)$ = $c_1x + c_1y$
            \hspace{1cm}
            $(c_1+c_2)x$ = $c_1x + c_2x$

            \begin{proof}[15cm]
                $c_1(x+y)$
                = $(c_1x_1+c_1y_1 , ... , c_1x_n+c_1y_n)$
                = $(c_1x_1 , ... , c_1x_n)$ + $(c_1y_1 , ... , c_1y_n)$
                = $c_1x + c_1y$
                
                $(c_1+c_2)x$
                = $(c_1x_1+c_2x_1 , ... , c_1x_n+c_2x_n)$

                \hspace{1.8cm}
                = $(c_1x_1 , ... , c_1x_n)$ + $(c_2x_1 , ... , c_2x_n)$
                = $c_1x + c_2x$
            \end{proof}

        \item {\color{lgreen} Multiplicative Associativity}
        
            \hspace{0.5cm}
            $c_1(c_2)x$ = $(c_1c_2)x$

            \begin{proof}[15cm]
                $c_1(c_2)x$
                = $(c_1c_2x_1,...,c_1c_2x_n)$
                = $(c_1c_2)x$
            \end{proof}

        \item {\color{lgreen} Multiplicative Identity}
        
            \hspace{0.5cm}
            1x = x

            \begin{proof}[15cm]
                1x
                = $(1x_1,...,1x_n)$
                = $(x_1,...,x_n)$
                = x
            \end{proof}
    \end{enumerate}

    



\subsection{ Matrices }

    \begin{definition}{Matrix}{16cm}
        A $m \times n$ {\color{lblue} matrix} A $\in$ $M_{m \times n}(\mathbb{R})$:

        \hspace{0.5cm}
        $\begin{bmatrix}
            a_{11} & a_{12} & \hdots & a_{1n} \\
            a_{21} & a_{22} & \hdots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \hdots & a_{mn}
        \end{bmatrix}$
        \hspace{0.5cm}
        where each $a_{ij}$ $\in$ $\mathbb{R}$

        Let 0 $\in$ $M_{m \times n}(\mathbb{R})$:
        be a m $\times$ n matrix where the value of all entries are 0.

        For A,B $\in$ $M_{m \times n}(\mathbb{R})$
        and scalar c $\in$ $\mathbb{R}$, define:

        \hspace{0.5cm}
        A + B =
        $\begin{bmatrix}
            a_{11}+b_{11} & a_{12}+b_{12} & \hdots & a_{1n}+b_{1n} \\
            a_{21}+b_{21} & a_{22}+b_{22} & \hdots & a_{2n}+b_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1}+b_{m1} & a_{m2}+b_{m2} & \hdots & a_{mn}+b_{mn}
        \end{bmatrix}$

        \hspace{0.5cm}
        cA =
        $\begin{bmatrix}
            ca_{11} & ca_{12} & \hdots & ca_{1n} \\
            ca_{21} & ca_{22} & \hdots & ca_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            ca_{m1} & ca_{m2} & \hdots & ca_{mn}
        \end{bmatrix}$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Properties of Matrices}{16cm}
        For any A,B,C $\in$ $M_{m \times n}(\mathbb{R})$
        and $c_1,c_2$ $\in$ $\mathbb{R}$:
    \end{wtheorem}

    \begin{enumerate}[label=(\alph*), leftmargin=2cm, itemsep=0.1cm]
        \item {\color{lgreen} Commutativity}
        
            \hspace{0.5cm}
            A + B = B + A

            \begin{proof}[15cm]
                A + B =
                $\begin{bmatrix}
                    a_{11}+b_{11} & \hdots & a_{1n}+b_{1n} \\
                    \vdots  & \ddots & \vdots \\
                    a_{m1}+b_{m1} & \hdots & a_{mn}+b_{mn}
                \end{bmatrix}$ =
                $\begin{bmatrix}
                    b_{11}+a_{11} & \hdots & b_{1n}+a_{1n} \\
                    \vdots & \ddots & \vdots \\
                    b_{m1}+a_{m1} & \hdots & b_{mn}+a_{mn}
                \end{bmatrix}$
                = B + A
            \end{proof}

            \newpage

        \item {\color{lgreen} Addititive Associativity}
        
            \hspace{0.5cm}
            (A + B) + C = A + (B + C)

            \begin{proof}[15cm]
                (A + B) + C =
                $\begin{bmatrix}
                    a_{11}+b_{11}+c_{11} & \hdots & a_{1n}+b_{1n}+c_{1n} \\
                    \vdots  & \ddots & \vdots \\
                    a_{m1}+b_{m1}+c_{m1} & \hdots & a_{mn}+b_{mn}+c_{mn}
                \end{bmatrix}$ = A + (B + C)
            \end{proof}

        \item {\color{lgreen} Additive Identity}
        
            There exists a unique $0_M$ $\in$ $M_{m \times n}(\mathbb{R})$
            such that for all A $\in$ $M_{m \times n}(\mathbb{R})$:

            \hspace{0.5cm}
            $0_M$ + A = A

            Moreover, $0_M$ = 0.
            This only holds for $M_{m \times n}(\mathbb{R})$
            and not all vector spaces.

            \begin{proof}[15cm]
                Suppose there are $0_{M_1},0_{M_2}$ where
                $0_{M_1}$ + A = A and $0_{M_2}$ + A = A for all A. Then:

                \hspace{0.5cm}
                $0_{M_1}$
                = $0_{M_2}$ + $0_{M_1}$
                = $0_{M_1}$ + $0_{M_2}$
                = $0_{M_2}$

                Thus, $0_M$ must be unique.

                Since
                0 + A =
                $\begin{bmatrix}
                    0+a_{11} & \hdots & 0+a_{1n} \\
                    \vdots  & \ddots & \vdots \\
                    0+a_{m1} & \hdots & 0+a_{mn}
                \end{bmatrix}$ =
                $\begin{bmatrix}
                    a_{11} & \hdots & a_{1n} \\
                    \vdots  & \ddots & \vdots \\
                    a_{m1} & \hdots & a_{mn}
                \end{bmatrix}$
                = A, then $0_M$ = 0.
            \end{proof}

        \item {\color{lgreen} Additive Inverse}
        
            For any A, there exists a unique -A $\in$ $M_{m \times n}(\mathbb{R})$
            such that:

            \hspace{0.5cm}
            A + (-A) = $0_M$

            Moreover, $-A$ = $(-1)A$.
            This only holds for $M_{m \times n}(\mathbb{R})$
            and not all vector spaces.

            \begin{proof}[15cm]
                Suppose there are $(-A)_a,(-A)_b$ where
                A + $(-A)_a$ = $0_M$ and A + $(-A)_b$ = $0_M$.

                \hspace{0.5cm}
                $(-A)_a$
                = $(-A)_a + 0_M$
                = $(-A)_a + A + (-A)_b$
                = $0_M + (-A)_b$
                = $(-A)_b$

                Thus, $-M$ must be unique.

                Since
                (-1)A + A =
                $\begin{bmatrix}
                    -a_{11}+a_{11} & \hdots & -a_{1n}+a_{1n} \\
                    \vdots  & \ddots & \vdots \\
                    -a_{m1}+a_{m1} & \hdots & -a_{mn}+a_{mn}
                \end{bmatrix}$
                = 0,
                then $-A$ = $(-1)A$.
            \end{proof}

        \item {\color{lgreen} Distributivity}
        
            \hspace{0.5cm}
            $c_1(A+B)$ = $c_1A + c_1B$
            \hspace{1cm}
            $(c_1+c_2)A$ = $c_1A + c_2A$

            \begin{proof}[15cm]
                $c_1(A+B)$ =
                $\begin{bmatrix}
                    c_1a_{11}+c_1b_{11} & \hdots & c_1a_{1n}+c_1b_{1n} \\
                    \vdots & \ddots & \vdots \\
                    c_1a_{m1}+c_1b_{m1} & \hdots & c_1a_{mn}+c_1b_{mn}
                \end{bmatrix}$
                = $c_1A + c_1B$

                $(c_1+c_2)A$ =
                $\begin{bmatrix}
                    c_1a_{11}+c_2a_{11} & \hdots & c_1a_{1n}+c_2a_{1n} \\
                    \vdots & \ddots & \vdots \\
                    c_1a_{m1}+c_2a_{m1} & \hdots & c_1a_{mn}+c_2a_{mn}
                \end{bmatrix}$
                = $c_1A + c_2A$
            \end{proof}

        \item {\color{lgreen} Multiplicative Associativity}
        
            \hspace{0.5cm}
            $c_1(c_2)A$ = $(c_1c_2)A$

            \begin{proof}[15cm]
                $c_1(c_2)A$ =
                $\begin{bmatrix}
                    c_1c_2a_{11} & \hdots & c_1c_2a_{1n} \\
                    \vdots & \ddots & \vdots \\
                    c_1c_2a_{m1} & \hdots & c_1c_2a_{mn}
                \end{bmatrix}$
                = $(c_1c_2)A$
            \end{proof}

            \newpage

        \item {\color{lgreen} Multiplicative Identity}
        
            \hspace{0.5cm}
            1A = A

            \begin{proof}[15cm]
                1A =
                $\begin{bmatrix}
                    1a_{11} & \hdots & 1a_{1n} \\
                    \vdots & \ddots & \vdots \\
                    1a_{m1} & \hdots & 1a_{mn}
                \end{bmatrix}$ =
                $\begin{bmatrix}
                    a_{11} & \hdots & a_{1n} \\
                    \vdots & \ddots & \vdots \\
                    a_{m1} & \hdots & a_{mn}
                \end{bmatrix}$
                = A
            \end{proof}
    \end{enumerate}

    \vspace{0.5cm}





\subsection{ System of Equations }

    \begin{definition}{Elementary Row Operations}{16cm}
        There are three types of {\color{lblue} elementary row operations}:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item {\color{lgreen} Row Swapping}:
                Swapping two rows
    
            \item {\color{lgreen} Row Multiplication}:
                Multiplying a row by a nonzero scalar
    
            \item {\color{lgreen} Row Addition}:
                Add a multiple of a row to another row
        \end{enumerate}

        Note that for any elementary row operation, an entry in the i-th column
        can be be affected by another entry in the i-th column.

        \vspace{0.3cm}

        If for matrix A,B $\in$ $M_{m \times n}(\mathbb{R})$,
        there is a sequence of elementary row operations that
        transforms A to B, then A and B are {\color{lblue} row equivalent}.

        Note if there is a sequence that transforms A to B,
        then performing the sequence in reverse will transform B to A
        so row equivalence between A and B is the same as
        row equivalence between B and A.
    \end{definition}

    \vspace{0.5cm}



    \begin{definition}{Reduced Row-Echelon Form: RREF}{16cm}
        The {\color{lblue} reduced row-echelon form (rref)} of
        matrix A $\in$ $M_{m \times n}(\mathbb{R})$, rref(A) satisfies:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item If a row has nonzero entries, the first nonzero is 1
    
            \item If a row has a leading 1, then each row before it has a leading 1
    
            \item A column with a leading 1 has 0 for the other entries
        \end{enumerate}

        For example:

        \hspace{0.5cm}
        $\begin{bmatrix}
            \circnum{1} & 2 & 0 & 1 \\
            0 & 0 & \circnum{1} & -1 \\
            0 & 0 & 0 & 0
        \end{bmatrix}$
    \end{definition}

    \vspace{0.5cm}



    \begin{definition}{System of Equations: Augmented Matrix}{16cm}
        A m $\times$ n system of equations written as a
        m $\times$ (n+1) {\color{lblue} augmented matrix}
        A $\in$ $M_{m \times (n+1)}(\mathbb{R})$:

        \hspace{0.5cm}
        $\begin{matrix*}[l]
            a_{11}x_1 + a_{12}x_2 + \hdots + a_{1n}x_n = b_1 \\
            a_{21}x_1 + a_{22}x_2 + \hdots + a_{2n}x_n = b_2 \\
            \hdots \\
            a_{m1}x_1 + a_{m2}x_2 + \hdots + a_{mn}x_n = b_m
        \end{matrix*}$
        \hspace{0.5cm}
        $\Leftrightarrow$
        \hspace{0.5cm}
        A = 
        $\begin{bmatrix}
            a_{11} & a_{12} & \hdots & a_{1n} & | & b_1 \\
            a_{21} & a_{22} & \hdots & a_{2n} & | & b_2 \\
            \vdots & \vdots & \ddots & \vdots & | & \vdots \\
            a_{m1} & a_{m2} & \hdots & a_{mn} & | & b_m
        \end{bmatrix}$ 

        If the i-th column of the rref(A) has a leading 1, then $x_i$ is called
        a {\color{lblue} pivot variable} else it is called a
        {\color{lblue} free variable}. For example:

        \hspace{0.5cm}
        $\begin{bmatrix}
            \circnum{1} & 2 & 0 & | & 1 \\
            0 & 0 & \circnum{1} & | & -1 \\
            0 & 0 & 0 & | & 0
        \end{bmatrix}$
        \hspace{0.5cm}
        $\Leftrightarrow$
        \hspace{0.5cm}
        $\begin{matrix*}[l]
            x_1 + 2x_2 = 1 \\
            x_3 = -1
        \end{matrix*}$
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3
        \end{bmatrix}$ =
        $\begin{bmatrix}
            -2x_2 + 1 \\
            x_2 \\
            -1
        \end{bmatrix}$

        Note a pivot variable, \{$x_1,x_3$\}, has a fixed value
        or its value depends on the free variables while the free variables,
        \{$x_2$\}, can be any value.
    \end{definition}

    \newpage



    \begin{wtheorem}{Gauss-Jordan Elimination:
    Elementary Row Operations don't change solutions}{16cm}
        Let m $\times$ n system of equations be the
        augmented matrix A $\in$ $M_{m \times (n+1)}(\mathbb{R})$.
        By performing elementary row operations on A to get to rref(A),
        the solutions are unchanged.
    \end{wtheorem}

    \begin{proof}
        Suppose the i-th row is multiplied by scalar c.

        \hspace{0.5cm}
        \footnotesize
        $\begin{bmatrix}
            a_{11} & \hdots & a_{1n} & | & b_1 \\
            \vdots & \ddots & \vdots & | & \vdots \\
            ca_{i1} & \hdots & ca_{in} & | & cb_i \\
            \vdots & \ddots & \vdots & | & \vdots \\
            a_{m1} & \hdots & a_{mn} & | & b_m
        \end{bmatrix}$
        \hspace{0.5cm}
        $\Leftrightarrow$
        \hspace{0.5cm}
        $\begin{matrix*}[l]
            a_{11}x_1 + \hdots + a_{1n}x_n = b_1 \\
            \hdots \\
            ca_{i1}x_1 + \hdots + ca_{in}x_n = cb_i \\
            \hdots \\
            a_{m1}x_1 + \hdots + a_{mn}x_n = b_m
        \end{matrix*}$
        \normalsize

        \hspace{0.5cm}
        If $(x_1^*,...,x_n^*)$ is a solution,
        then $a_{i1}x_1^* + ... + a_{in}x_n^*$ = $b_i$
        for any i $\in$ \{1,...,m\}:

        \hspace{1cm}
        $ca_{i1}x_1^* + ... + ca_{in}x_n^*$
        = $c(a_{i1}x_1^* + ... + a_{in}x_n^*)$
        = $cb_i$
        
        \hspace{0.5cm}
        If $(x_1^/,...,x_n^/)$ is not a solution,
        then $a_{i1}x_1^/ + ... + a_{in}x_n^/$ $\not =$ $b_i$
        for any i $\in$ \{1,...,m\}:
        
        \hspace{1cm}
        $ca_{i1}x_1^/ + ... + ca_{in}x_n^/$
        = $c(a_{i1}x_1^/ + ... + a_{in}x_n^/)$
        $\not =$ $cb_i$
        

        Thus, row multiplication does not change the solutions.
        Note c is nonzero since if c = 0, then any $(x_1,...,x_n)$
        satisfies $ca_{i1}x_1 + ca_{i2}x_2 + \hdots + ca_{in}x_n$
        = 0 = $cb_i$ which includes non-solutions.

        \rule[0.1cm]{16.7cm}{0.01cm}

        Suppose the i-th row multiplied by c is added to the j-th row.

        \hspace{0.5cm}
        \footnotesize
        $\begin{bmatrix}
            a_{11} & \hdots & a_{1n} & | & b_1 \\
            \vdots & \ddots & \vdots & | & \vdots \\
            ca_{i1}+a_{j1} & \hdots & ca_{in}+a_{jn}
                & | & cb_i+b_j \\
            \vdots & \ddots & \vdots & | & \vdots \\
            a_{m1} & \hdots & a_{mn} & | & b_m
        \end{bmatrix}$
        \hspace{0.5cm}
        $\Leftrightarrow$
        \hspace{0.5cm}
        $\begin{matrix*}[l]
            a_{11}x_1 + \hdots + a_{1n}x_n = b_1 \\
            \hdots \\
            (ca_{i1}+a_{j1})x_1 + \hdots + (ca_{in}+a_{jn})x_n = cb_i+b_j  \\
            \hdots \\
            a_{m1}x_1 + \hdots + a_{mn}x_n = b_m
        \end{matrix*}$
        \normalsize

        \hspace{0.5cm}
        If $(x_1^*,...,x_n^*)$ is a solution,
        then $a_{i1}x_1^* + ... + a_{in}x_n^*$ = $b_i$
        for any i $\in$ \{1,...,m\}:
        
        \hspace{1cm}
        $(ca_{i1}+a_{j1})x_1^* + ... + (ca_{in}+a_{jn})x_n^*$
        = $c(a_{i1}^* + ... + a_{in}^*) + (a_{j1}^* + ... + a_{jn}^*)$
        = $cb_i + b_j$
        
        \hspace{0.5cm}
        If $(x_1^/,...,x_n^/)$ is not a solution,
        then $a_{i1}x_1^/ + ... + a_{in}x_n^/$ $\not =$ $b_i$
        for any i $\in$ \{1,...,m\}:
        
        \hspace{1cm}
        $(ca_{i1}+a_{j1})x_1^/ + ... + (ca_{in}+a_{jn})x_n^/$
        = $c(a_{i1}^/ + ... + a_{in}^/) + (a_{j1}^/ + ... + a_{jn}^/)$
        $\not =$ $cb_i + b_j$

        Thus, row addition does not change the solutions.

        \rule[0.1cm]{16.7cm}{0.01cm}

        Suppose the i-th row is swapped with the j-th row.
        Note row swapping is the same as:

        \hspace{0.5cm}
        Add i-th row to j-th
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        j'-th = i-th + j-th

        \hspace{0.5cm}
        Subtract i-th row by j'-th
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        i'-th = -j-th

        \hspace{0.5cm}
        Add i'-th row to j'-th.
        Multiply the i'-th row by -1
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        j''-th = i-th
        \hspace{0.5cm}
        i''-th = j-th

        Since each step does not change solutions,
        then row swapping does not change solutions.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Row equivalent matrices have the same solutions}{16cm}
        If A,B $\in$ $M_{m \times n}(\mathbb{R})$
        are row equivalent, then Ax = 0 and Bx = 0
        have the same solutions
    \end{wtheorem}

    \begin{proof}
        If A and B are row equivalent, then the augmented matrices
        $[A \ | \ 0]$,$[B \ | \ 0]$ $\in$ $M_{m \times (n+1)}(\mathbb{R})$
        are row equivalent.
        Then, there is a sequence of elementary
        row operations that transforms $[A \ | \ 0]$ to $[B \ | \ 0]$.
        By {\color{red} theorem 1.3.4}, the solutions to $[A \ | \ 0]$
        don't change when transforming to $[B \ | \ 0]$
        so the solutions to $[A \ | \ 0]$ and $[B \ | \ 0]$ are the same.

        Note Ax,Bx = 0 since if Ax = b where b $\not =$ 0
        and Ax = c where c $\not =$ 0,
        then performing elementary row operations to change A to B might
        not change b to c. But if b = 0, then any elementary row operation will
        keep b as 0 since the entries in b can only be affected by other entries
        in b which are all 0. Thus, if also c = 0, then $[A \ | \ 0]$
        and $[B \ | \ 0]$ will be row equivalent.
    \end{proof}

    \newpage


    
    \begin{wtheorem}{The rref(A) is unique}{16cm}
        Let matrix A $\in$ $M_{m \times n}(\mathbb{R})$
        be row equivalent to matrix B,C $\in$ $M_{m \times n}(\mathbb{R})$
        which are in reduced row-echelon form. Then, B = C.
    \end{wtheorem}

    \begin{proof}
        Since A is row equivalent to B,C, then by {\color{red} theorem 1.3.5},
        Ax = 0 and Bx = 0 have the same solutions and
        Ax = 0 and Cx = 0 have the same solutions. Thus,
        Bx = 0 and Cx = 0 have the same solutions.
        The following proof will be a proof by induction.

        Suppose A,B,C $\in$ $M_{m \times 1}(\mathbb{R})$.
        Since B,C are in reduced row-echelon form, then B,C are either:

        \hspace{0.5cm}
        \footnotesize
        $M_1$ =
        $\begin{bmatrix}
            1 \\
            0 \\
            \vdots \\
            0
        \end{bmatrix}$
        \hspace{0.5cm}
        $M_2$ =
        $\begin{bmatrix}
            0 \\
            0 \\
            \vdots \\
            0
        \end{bmatrix}$

        \normalsize
        Since $M_1x$ = 0 is only x = 0 and
        $M_2x$ = 0 is any x $\in$ $\mathbb{R}$
        then either B,C = $M_1$ or B,C = $M_2$
        since Bx = 0 and Cx = 0 has the same solutions.
        Thus, the base case holds true.

        Suppose for some n $\in$ $\mathbb{Z_+}$,
        any matrix M $\in$ $M_{m \times n}(\mathbb{R})$
        in reduced row echelon form is unique.

        Let A,B,C $\in$ $M_{m \times (n+1)}(\mathbb{R})$
        where A is row equivalent to B,C in reduced row echelon form.
        Let $A_n,B_n,C_n$ $\in$ $M_{m \times n}(\mathbb{R})$
        be A,B,C without their (n+1)-th column.
        Since A is row equivalent to B,C, then $A_n$ is row equivalent
        to $B_n,C_n$ which are also in reduced row-echelon form since
        removing the last column of any rref is still a rref.
        Since $A_n$ $\in$ $M_{m \times n}(\mathbb{R})$
        which is row equivalent to
        reduced row echelon matrices $B_n,C_n$ $\in$ $M_{m \times n}(\mathbb{R})$,
        then $B_n$ = rref($A_n$) = $C_n$.
        Thus, the first n columns of B,C are the same.
        Suppose the B $\not =$ C so only the (n+1)-th column can be different.
        Then there is a i $\in$ \{1,...,m\} where
        $b_{i(n+1)}$ $\not =$ $c_{i(n+1)}$.
        Let $(x_1^*,...,x_{n+1}^*)$ be a solution.

        \hspace{0.5cm}
        $b_{i1}x_1 + ... + b_{in}x_n + b_{i(n+1)}x_{n+1}$ = 0
        \hspace{1cm}
        $c_{i1}x_1 + ... + c_{in}x_n + c_{i(n+1)}x_{n+1}$ = 0

        Since $B_n$ = $C_n$, then $b_{ij}$ = $c_{ij}$ for i = \{1,...,m\}
        and j = \{1,...,n\}. Thus,
        $b_{i(n+1)}x_{n+1}$ = $c_{i(n+1)}x_{n+1}$.

        Since $b_{i(n+1)}$ $\not =$ $c_{i(n+1)}$,
        then $x_{n+1}$ = 0. Thus, $x_{n+1}$ is a pivot variable so the (n+1)-th
        column of B,C have a leading 1. Thus, any other entry in the (n+1)-th
        column is 0. Since B and C are in reduced row-echelon form, then this
        pivot is right after any other pivots before it since this 1 is in
        the final column, but since the entries of $B_n,C_n$ are the same,
        then this 1 is in the same row in C as in B. Thus, the (n+1)-th column
        of B and C are the same which contradicts the assumption that
        (n+1)-th column are different. Thus, B = C
        by induction.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Number of Solutions in a System of Equations}{16cm}
        Let m $\times$ n system of equations be the
        augmented matrix A $\in$ $M_{m \times (n+1)}(\mathbb{R})$.
        The system is called {\color{lblue} consistent} if there
        is at least one solution and {\color{lblue} inconsistent}
        if there are no solutions.

        \vspace{0.2cm}

        If the rref(A) contains the row $[0 ... 0 \ | \ 1]$, then
        the system has no solutions.

        If there is at least one free variable, then there are infinitely
        many solutions and if all variables are pivots,
        then there is one solution.
    \end{wtheorem}

    \begin{proof}
        Since a variable is either a pivot or free variable,
        then the rref(A) either:
        
        \hspace{0.5cm}
        - contains the row $[0 ... 0 \ | \ 1]$

        \hspace{0.5cm}
        - doesn't contains the row $[0 ... 0 \ | \ 1]$
        and have all pivot variables

        \hspace{0.5cm}
        - doesn't contains the row $[0 ... 0 \ | \ 1]$,
        but have all pivot variables

        Since $[0 ... 0 \ | \ 1]$ implies 0 = $0x_1 + ... + 0x_n$ = 1,
        then if rref(A) contains the row $[0 ... 0 \ | \ 1]$,
        there cannot be any solution regardless of pivot and free
        variables since no x = $(x_1,...,x_n)$ will satisfy such a row.
        Now, suppose rref(A) doesn't contains the row $[0 ... 0 \ | \ 1]$.
        
        Suppose the rref(A) have all pivot variables.
        Since pivot variables are fixed or depend on free
        variables which don't exist, then the pivot variables are all fixed
        and thus, unique.

        Suppose the rref(A) has at least one free variable.
        Then at least one variable can be any real number and thus,
        there are infinitely many solutions.
    \end{proof}

    \newpage



    \begin{corollary}{A unique solution must have as many equation as there
    are unknowns}{16cm}
        Let m $\times$ n system of equations be the
        augmented matrix A $\in$ $M_{m \times (n+1)}(\mathbb{R})$.

        If there is a unique solution, then m $\geq$ n.
    \end{corollary}

    \begin{proof}
        By {\color{red} theorem 1.3.7}, a unique solution must
        have all pivot variables. If rref(A) has all pivots,
        then m $\geq$ n else there will be a column without a pivot.
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Homogeneous \& Inhomogeneous Equations}{16cm}
        A m $\times$ n system of equations: 

        \hspace{0.5cm}
        $\begin{matrix*}[l]
            a_{11}x_1 + a_{12}x_2 + \hdots + a_{1n}x_n = b_1 \\
            a_{21}x_1 + a_{22}x_2 + \hdots + a_{2n}x_n = b_2 \\
            \hdots \\
            a_{m1}x_1 + a_{m2}x_2 + \hdots + a_{mn}x_n = b_m
        \end{matrix*}$
        
        can also be written in {\color{lblue} matrix form}:

        \hspace{0.5cm}
        Ax = b

        \hspace{0.5cm}
        where A =
        $\begin{bmatrix}
            a_{11} & a_{12} & \hdots & a_{1n} \\
            a_{21} & a_{22} & \hdots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \hdots & a_{mn}
        \end{bmatrix}$ $\in$ $M_{m \times n}(\mathbb{R})$,
        \hspace{0.2cm}
        x = 
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n
        \end{bmatrix}$ $\in$ $\mathbb{R}^n$,
        \hspace{0.2cm}
        b =
        $\begin{bmatrix}
            b_1 \\
            b_2 \\
            \vdots \\
            b_m
        \end{bmatrix}$ $\in$ $\mathbb{R}^m$

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item {\color{lgreen} Closed under Addition}:
                For any $x_1,...x_k$ $\in$ $\mathbb{R}^n$:

                \hspace{0.5cm}
                $A(x_1+...+x_k)$ = $Ax_1 + ... + Ax_k$

            \item {\color{lgreen} Closed under Scalar Multiplication}
                For any x $\in$ $\mathbb{R}^n$ and c $\in$ $\mathbb{R}$:

                \hspace{0.5cm}
                $A(cx)$ = $c Ax$
        \end{enumerate}

        \begin{boxedminipage}{16cm}
            For $x_1,...,x_k$ $\in$ $\mathbb{R}^n$
            and $c_1,...,c_k$ $\in$ $\mathbb{R}$:
    
            \hspace{0.5cm}
            $A(c_1x_1+...+c_kx_k)$
    
            \hspace{0.5cm}
            \rule[0.1cm]{15.1cm}{0.01cm}
    
            \hspace{0.5cm}
            $\Leftrightarrow$
            $\begin{matrix*}[l]
                a_{11}(c_1x_{11}+...+c_kx_{k1})
                    + a_{12}(c_1x_{12}+...+c_kx_{k2})
                    + \hdots + a_{1n}(c_1x_{1n}+...+c_kx_{kn}) \\
                a_{21}(c_1x_{11}+...+c_kx_{k1})
                    + a_{22}(c_1x_{12}+...+c_kx_{k2})
                    + \hdots + a_{2n}(c_1x_{1n}+...+c_kx_{kn}) \\
                ... \\
                a_{m1}(c_1x_{11}+...+c_kx_{k1})
                    + a_{m2}(c_1x_{12}+...+c_kx_{k2})
                    + \hdots + a_{mn}(c_1x_{1n}+...+c_kx_{kn}) \\
            \end{matrix*}$
    
            \hspace{0.5cm}
            \rule[0.1cm]{15.1cm}{0.01cm}
    
            \hspace{0.5cm}
            =
            $\begin{matrix*}[l]
                c_1(a_{11}x_{11} + ... + a_{1n}x_{1n}) \\
                c_1(a_{21}x_{11} + ... + a_{2n}x_{1n}) \\
                ... \\
                c_1(a_{m1}x_{11} + ... + a_{mn}x_{1n}) \\
            \end{matrix*} + ... +
            \begin{matrix*}[l]
                c_k(a_{11}x_{k1} + ... + a_{1n}x_{kn}) \\
                c_k(a_{21}x_{k1} + ... + a_{2n}x_{kn}) \\
                ... \\
                c_k(a_{m1}x_{k1} + ... + a_{mn}x_{kn}) \\
            \end{matrix*}$
            $\Leftrightarrow$ $c_1Ax_1 + ... + c_kAx_k$
        \end{boxedminipage}

        \vspace{0.3cm}

        A {\color{lblue} Homogeneous equation} is in the form:

        \hspace{0.5cm}
        Ax = 0
        \hspace{1cm}
        where A $\in$ $M_{m \times n}(\mathbb{R})$,
        x $\in$ $\mathbb{R}^n$,
        and 0 $\in$ $\mathbb{R}^m$

        A {\color{lblue} Inhomogeneous equation} is in the form:

        \hspace{0.5cm}
        Ax = b
        \hspace{1cm}
        where A $\in$ $M_{m \times n}(\mathbb{R})$,
        x $\in$ $\mathbb{R}^n$,
        and b $\not =$ 0 $\in$ $\mathbb{R}^m$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Relationship between Homogeneous and Inhomogeneous}{16cm}
        Let $x_0$ be a solution to Ax = b. Then any solution $x_s$ of Ax = b:

        \hspace{0.5cm}
        $x_s$ = $x_0 + x^*$

        where $x^*$ is a solution to Ax = 0
    \end{wtheorem}

    \begin{proof}
        Let $x_0$ be a solution to Ax = b. Suppose $x_s$ be a solution to Ax = b.

        \hspace{0.5cm}
        b = $Ax_s$
        = $A(x_0 + (x_s - x_0))$
        = $Ax_0 + A(x_s - x_0)$
        = b + $A(x_s - x_0)$
        
        Thus, $A(x_s - x_0)$ = 0
        so $x^*$ = $x_s - x_0$ is a solution to Ax = 0.
    \end{proof}

    \newpage



    



    \begin{example}
        Find the solution(s), $(x_1,x_2,x_3,x_4)$:

        \hspace{0.5cm}
        $\begin{matrix*}[l]
            x_1 + 2x_3 + 4x_4 = -8 \\
            x_2 - 3x_3 - x_4 = 6 \\
            3x_1 + 4x_2 - 6x_3 + 8x_4 = 0 \\
            -x_2 + 3x_3 + 4x_4 = - 12
        \end{matrix*}$

        What are the solutions if instead the equations are equal to 0?
    \end{example}

    \begin{tbox}
        \small
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & -8 \\
            0 & 1 & -3 & -1 & | & 6 \\
            3 & 4 & -6 & 8 & | & 0 \\
            0 & -1 & 3 & 4 & | & -12
        \end{bmatrix}$
        \normalsize
        $\underset{\Rightarrow}{\substack{\text{Add -3(1st) to the (3rd)}}}$
        \small
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & -8 \\
            0 & 1 & -3 & -1 & | & 6 \\
            0 & 4 & -12 & -4 & | & 24 \\
            0 & -1 & 3 & 4 & | & -12
        \end{bmatrix}$

        \normalsize
        $\underset{\Rightarrow}{\substack{\text{Add -4(2nd) to the (3rd)}
                                        \\ \text{Add (2nd) to the (4th)}}}$
        \small
        $\begin{bmatrix}
            1 & 0 & 2 & 4 & | & -8 \\
            0 & 1 & -3 & -1 & | & 6 \\
            0 & 0 & 0 & 0 & | & 0 \\
            0 & 0 & 0 & 3 & | & -6
        \end{bmatrix}$
        \normalsize
        $\underset{\Rightarrow}{\substack{\text{Multiply (4th) by $\frac{1}{3}$}
                                        \\ \text{Add -4(4th) to (1st)}
                                        \\ \text{Add (4th) to (2nd)}}}$
        \small
        $\begin{bmatrix}
            1 & 0 & 2 & 0 & | & 0 \\
            0 & 1 & -3 & 0 & | & 4 \\
            0 & 0 & 0 & 0 & | & 0 \\
            0 & 0 & 0 & 1 & | & -2
        \end{bmatrix}$

        \normalsize
        Thus, the reduced row-echelon form of this matrix:

        \hspace{0.5cm}
        \small
        $\begin{bmatrix}
            1 & 0 & 2 & 0 & | & 0 \\
            0 & 1 & -3 & 0 & | & 4 \\
            0 & 0 & 0 & 1 & | & -2 \\
            0 & 0 & 0 & 0 & | & 0
        \end{bmatrix}$

        \normalsize
        The pivot variables are $x_1,x_2,x_4$
        and the free variable is $x_3$.
        The solutions are:

        \hspace{0.5cm}
        \small
        $\begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3 \\
            x_4
        \end{bmatrix}$ =
        $\begin{bmatrix}
            -2x_3 \\
            3x_3 + 4 \\
            x_3 \\
            -2
        \end{bmatrix}$ =
        $\begin{bmatrix}
            -2x_3 \\
            3x_3 \\
            x_3 \\
            0
        \end{bmatrix} +
        \begin{bmatrix}
            0 \\
            4 \\
            0 \\
            -2
        \end{bmatrix}$ =
        $x_3\begin{bmatrix}
            -2 \\
            3 \\
            1 \\
            0
        \end{bmatrix} +
        \begin{bmatrix}
            0 \\
            4 \\
            0 \\
            -2
        \end{bmatrix}$
        = $x_3(-2,3,1,0) + (0,4,0,-2)$

        \normalsize
        Thus, $x_3(-2,3,1,0)$
        are the solutions when the equations equal to 0.
    \end{tbox}




